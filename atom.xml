<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>QF的个人博客</title>
  
  <subtitle>忍把浮名,换了浅斟低唱</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://blog.logan-qiu.cn/"/>
  <updated>2019-12-11T16:39:27.101Z</updated>
  <id>http://blog.logan-qiu.cn/</id>
  
  <author>
    <name>QF</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>编译原理复习整理（一）</title>
    <link href="http://blog.logan-qiu.cn/posts/b4e4b501/"/>
    <id>http://blog.logan-qiu.cn/posts/b4e4b501/</id>
    <published>2019-12-11T13:47:36.000Z</published>
    <updated>2019-12-11T16:39:27.101Z</updated>
    
    <content type="html"><![CDATA[<p>课程来自哈尔滨工业大学的的 <a href="https://www.icourse163.org/learn/HIT-1002123007#/learn/announce" target="_blank" rel="noopener">《编译原理》</a></p><h2 id="字母表"><a href="#字母表" class="headerlink" title="字母表"></a>字母表</h2><p>字母表$\Sigma$十一个有穷符号集合。 （符号包括字母、数字、标点符号）  </p><p>二进制字母表$\{0,1\}$，ASCII字符集，Unicode字符集都属于字母表。</p><h3 id="字母表的运算"><a href="#字母表的运算" class="headerlink" title="字母表的运算"></a>字母表的运算</h3><ul><li>乘积： $\Sigma_1\Sigma_2=\{a,b|a\in\Sigma_1, b\in\Sigma_2\}$  Ex: $\{0,1\}\{a,b\}=\{0a,0b,1a,1b\}$</li><li><p>幂：$\begin{equation}\left\{\begin{array}{lr}\Sigma^0 = \{\epsilon\}(空串) &amp; \\ \Sigma^n=\Sigma^{n-1}\Sigma\end{array}\right.\end{equation}$  Ex: $\{0,1\}^3 = \{000,001,010,011,100,101,110,111\}$</p></li><li><p>正闭包：$\Sigma^+=\Sigma\bigcup\Sigma^2\bigcup\Sigma^3\bigcup\dots$   Ex: $\{a,b,c,d\}^+=\{a,b,c,d,aa,ab,ac,ad,ba,bb,bc,bd,ca,cb,cc,cd,da,db,dc,dd,aaa,\dots\}$</p></li><li><p>（克林）闭包：$\Sigma^*=\Sigma^0\bigcup\Sigma^+$   就是在正闭包的基础上加上一个空串。</p></li></ul><h2 id="串"><a href="#串" class="headerlink" title="串"></a>串</h2><p>设$\Sigma$是一个字母表，$\forall{x\in\Sigma^*}$，称$x$为$\Sigma$上的一个串，串是字母表中符号的一个有穷序列。  </p><p>串$s$的长度通常记为$|s|$，指$s$中符号的个数。   Ex: $|aab|=3$  </p><h3 id="串上的运算"><a href="#串上的运算" class="headerlink" title="串上的运算"></a>串上的运算</h3><ul><li><p>连接：$xy$ ， $x=dog, y=house, xy=doghouse$</p><p>对于空串：$\epsilon s=s\epsilon = s$  </p><p>设$x,y,z$是三个字符串，若$x = yz$，则$y$为$x$的前缀，$z$为$x$的后缀。</p></li><li><p>幂运算：$\begin{equation}\left\{\begin{array}{lr}s^0 = \epsilon, &amp; \\ s^n=s^{n-1}s, &amp;n\geq1\end{array}\right.\end{equation}$    Ex: $s=ba,s^1=ba,s^2=baba,s^3=bababa$</p></li></ul><h2 id="正则表达式-RE"><a href="#正则表达式-RE" class="headerlink" title="正则表达式(RE)"></a>正则表达式(RE)</h2><p>语言可以用集合表示，设语言<script type="math/tex">L = \{a\}\{a,b\}^*(\{\epsilon\}\bigcup(\{.,\_\}\{a,b\}\{a,b\}^*))</script></p><p>正则表达式是一种用来描述正则语言的更紧凑的表示方法。  </p><p>例如<script type="math/tex">r=a(a|b)^*(\epsilon|(.|\_)(a|b)(a|b)^*)</script></p><p>正则表达式可以由较小的正则表达式按照特定规则递归的构建。每个正则表达式$r$定义（表示）一个语言，记为$L(r)$。这个语言是根据$r$的字表达式递归定义的。</p><h3 id="正则表达式的定义"><a href="#正则表达式的定义" class="headerlink" title="正则表达式的定义"></a>正则表达式的定义</h3><ul><li>$\epsilon$是一个RE，$L(\epsilon) = \epsilon$</li><li>如果$a\in\Sigma$，则$a$是一个RE，$L(a) = \{a\}$</li><li>假设$r$和$s$都是RE，表示的语言分别为$L(r)$和$L(s)$，则：<ul><li>$r|s$是一个RE，$L(r|s)=L(r)\bigcup L(s)$</li><li>$rs$是一个RE，L(rs) = L(r)L(s)$</li><li>$r^<em>$是一个RE，L(r^</em>)=L(r)^*$</li><li>$(r)$是一个RE，L((r))=L(r)$ </li></ul></li></ul><p>运算优先级: $* &gt; $ 连接 $&gt; |$</p><h3 id="正则定义"><a href="#正则定义" class="headerlink" title="正则定义"></a>正则定义</h3><p>正则定义就相当于一个函数封装。</p><h2 id="有穷自动机（FA）"><a href="#有穷自动机（FA）" class="headerlink" title="有穷自动机（FA）"></a>有穷自动机（FA）</h2><h3 id="转换图"><a href="#转换图" class="headerlink" title="转换图"></a>转换图</h3><p>初始状态用$\Rightarrow$表示, 终止状态用双圈表示。  </p><p>由一个有穷自动机$M$接收的所有串构成的集合称为是FA定义（或接收）的语言，记为$L(M)$。  </p><p><img alt="有穷自动机" data-src="/posts/b4e4b501/fa.png" class="lazyload"></p><h3 id="最长字串匹配原则"><a href="#最长字串匹配原则" class="headerlink" title="最长字串匹配原则"></a>最长字串匹配原则</h3><p>当输入串的多个前缀与一个或者多个模式匹配时，总是选择最长的前缀进行匹配。  </p><p>说人话就是到了终止状态，若还有符号，则继续前进。</p><h3 id="确定的有穷自动机（DFA）"><a href="#确定的有穷自动机（DFA）" class="headerlink" title="确定的有穷自动机（DFA）"></a>确定的有穷自动机（DFA）</h3><script type="math/tex; mode=display">M = (S, \Sigma,\delta,s_0, F)</script><p>$S$：有穷状态集</p><p>$\Sigma$：输入字母表</p><p>$\delta$：将$S\times\Sigma$映射到$S$的转换函数，$\delta(s,a)$表示从s出发，沿a能到达的状态</p><p>$s_0$：开始状态（或初始状态），$s_0\in S$</p><p>$F$：接收状态（或终止状态）集合，$F\subseteq S$</p><p>DFA也可以用转换表来表示</p><p><img alt="dfa" data-src="/posts/b4e4b501/dfa.png" class="lazyload">  </p><h3 id="不确定的有穷自动机"><a href="#不确定的有穷自动机" class="headerlink" title="不确定的有穷自动机"></a>不确定的有穷自动机</h3><script type="math/tex; mode=display">M = (S, \Sigma,\delta,s_0, F)</script><p>和DFA的区别在于，$\delta(s,a)$表示从s出发，沿a能到达的<strong>所有状态的集合</strong>。  </p><p><img alt="nfa" data-src="/posts/b4e4b501/nfa.png" class="lazyload"></p><h3 id="NFA和DFA的等价性"><a href="#NFA和DFA的等价性" class="headerlink" title="NFA和DFA的等价性"></a>NFA和DFA的等价性</h3><p><img alt="NFA和DFA的等价性" data-src="/posts/b4e4b501/nfaeqdfa.png" class="lazyload"></p><p>在上图的DFA中，状态1表示串以a结尾，状态2表示串以ab结尾，状态3表示串以abb结尾。  </p><p>他们都是$r=(a|b)^*abb$，由此$RE \Leftrightarrow FA \Leftrightarrow 正则文法$。</p><h3 id="带有和不带有-epsilon-边的NFA的等价性"><a href="#带有和不带有-epsilon-边的NFA的等价性" class="headerlink" title="带有和不带有$\epsilon$边的NFA的等价性"></a>带有和不带有$\epsilon$边的NFA的等价性</h3><p><img alt data-src="/posts/b4e4b501/knfa.png" class="lazyload"></p><h3 id="从正则表达式到有穷自动机"><a href="#从正则表达式到有穷自动机" class="headerlink" title="从正则表达式到有穷自动机"></a>从正则表达式到有穷自动机</h3><h4 id="根据RE构造NFA"><a href="#根据RE构造NFA" class="headerlink" title="根据RE构造NFA"></a>根据RE构造NFA</h4><p><img alt data-src="/posts/b4e4b501/re2nfa1.png" class="lazyload"></p><p><img alt data-src="/posts/b4e4b501/re2nfa2.png" class="lazyload"></p><p><img alt data-src="/posts/b4e4b501/re2nfa3.png" class="lazyload"></p><h4 id="从NFA到DFA的转换"><a href="#从NFA到DFA的转换" class="headerlink" title="从NFA到DFA的转换"></a>从NFA到DFA的转换</h4><p>子集构造法。</p><p><img alt data-src="/posts/b4e4b501/nfa2dfa1.png" class="lazyload"></p><p><img alt data-src="/posts/b4e4b501/nfa2dfa2.png" class="lazyload"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;课程来自哈尔滨工业大学的的 &lt;a href=&quot;https://www.icourse163.org/learn/HIT-1002123007#/learn/announce&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;《编译原理》&lt;/a&gt;&lt;/p&gt;
&lt;h2
      
    
    </summary>
    
    
      <category term="编译原理" scheme="http://blog.logan-qiu.cn/categories/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/"/>
    
    
      <category term="编译原理" scheme="http://blog.logan-qiu.cn/tags/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/"/>
    
      <category term="期末复习" scheme="http://blog.logan-qiu.cn/tags/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>DenseNet论文阅读笔记</title>
    <link href="http://blog.logan-qiu.cn/posts/ef2be802/"/>
    <id>http://blog.logan-qiu.cn/posts/ef2be802/</id>
    <published>2019-12-08T05:01:31.000Z</published>
    <updated>2019-12-11T10:22:48.836Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/pdf/1608.06993.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1608.06993.pdf</a></p><p>官方实现和预训练模型：<a href="https://github.com/liuzhuang13/DenseNet" target="_blank" rel="noopener">https://github.com/liuzhuang13/DenseNet </a></p><h2 id="相关背景"><a href="#相关背景" class="headerlink" title="相关背景"></a>相关背景</h2><p>本篇在16年8月挂到arXiv上，中了2017年CVPR，是继16年何大神的ResNet之后，第二个华人的best paper，<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F53417625" target="_blank" rel="noopener">这里</a>有个作者本尊的talk，现场讲解。一作Gao Huang（黄高）05年北航的本科生（GPA第一），15年清华博士毕业（读了6年。。），后来在康奈尔待了3年做博后，此刻在清华作青椒，本篇是在康奈尔时的工作。二作刘壮（同等贡献）也是碉堡，现在在伯克利做博士生，之前是清华姚班的（13级），发这篇文章时还在清华，也就是说<a href="https://links.jianshu.com/go?to=http%3A%2F%2Fiiis.tsinghua.edu.cn%2Fshow-6425-1.html" target="_blank" rel="noopener">本科生</a>。。。最近以一作的身份新发了一篇《Rethinking the Value of Network Pruning》，中了19年的ICLR，同时也是18年NIPS的best paper award。。这个世界太疯狂了，这都不是潜力股了，而是才华横溢溢的不行了。$^{[1]}$</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>作者说在之前的一些研究中，发现了在卷积神经网络中增加一些捷径（<em>shorter connections</em>，看过ResNet的应该明白）。DenseNet提出来的新的方法是，把所有层提取出来的feature map都传递到后面的层中。同时，作者提出了DenseNet的一些优势：缓解梯度消失、加强特征传递、特征复用和显著减少训练参数。他们将DenseNet应用于一些物体识别 的任务，发现在使用更少参数的情况下还能获得比其他网络更好地效果。</p><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>这篇文章的引言部分呢，把摘要复读了一遍。不过在第三段讲DenseNet和ResNet区别的时候，提出了一个concatenating的概念。我第一次读的时候在这个词的理解上花费了很大的功夫。其实这个concatenating 就是把上层的所有feature map直接连接在下层的feature map后面，这样的话每一层都会包含之前的所有信息。</p><h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h2><p>作者说，对网络结构的探索是神经网络研究的重要工作。Highway Networks和ResNet让网络go deeper，而Inception Network能让网络go wider。而DenseNet使用的是完全不同的方法，作者强调特征复用，制造更简单有效的网络。$^{[2]}$</p><h2 id="3-DenseNet"><a href="#3-DenseNet" class="headerlink" title="3. DenseNet"></a>3. DenseNet</h2><p>从这里开始，正式介绍DenseNet，是这篇文章的干货。</p><p>作者假设有一张图片$x_0$被输入到一个L层的卷积神经网络中，这个网络的每一层都使用了一种非线性变换$H_l(·)$中。这个$H_l(·)$是实现了Batch Normalization，ReLU，池化和卷积之类的操作的复合函数。假设每一层的输出为$x_l$。</p><h3 id="3-1-ResNets"><a href="#3-1-ResNets" class="headerlink" title="3.1 ResNets"></a>3.1 ResNets</h3><p>其实这一部分是和4.2对比着写的，也是为了突出DenseNet的不同之处。传统的卷积网络把第$l$层的输出作为$l+1$层的输入。ResNet增加了一个跳跃连接，就相当于$x_l = H_l(x_{l-1}) + x_{l-1}$ 。作者这么说，其实就是相当于用自定义的$H_l(·)$函数来解释一些下ResNet里面的那个$a_{l+2} = g(z_{l+2} + a_l)$。也就是说，在ResNet中，复合函数$H_l(·)$就是一个Conv+Pool+ReLu+Conv+Pool。</p><h3 id="3-2-Dense-connectivityxw"><a href="#3-2-Dense-connectivityxw" class="headerlink" title="3.2 Dense connectivityxw"></a>3.2 Dense connectivityxw</h3><p>这一部分是作者提出DenseNet的连接方式，我们看作者提出的式子:</p><script type="math/tex; mode=display">x_l = H_l([x_0, x_1, x_2, \dots, x_{l-1}]) \tag{1}</script><p>先不管$H_l(·)$的具体实现，这个式子可以说明每一层的特征图都会包含之前所有特征。同时，观察图3.2.1和图3.2.2，由于DenseNet强调特征复用，所以他的每一个卷积层能够变得更“slim”， 这也是DenseNet所用的参数比较少的一个原因。对于图3.2.3，我们可以发现，对于$L$层的网络，DenseNet总共需要$\frac{L(L+1)}{2}$次连接。</p><p><img alt="图3.2.1 直接连接方式" data-src="/posts/ef2be802/standardconnect.png" class="lazyload"></p><p><img alt="图3.2.2 Dense连接1" data-src="/posts/ef2be802/denseconnect.png" class="lazyload"></p><p><img alt="图3.2.3 Dense连接2" data-src="/posts/ef2be802/cover.png" class="lazyload"></p><h3 id="3-3-Composite-function"><a href="#3-3-Composite-function" class="headerlink" title="3.3 Composite function"></a>3.3 Composite function</h3><p>这个部分作者对复合函数进行了非常简短的说明，$H_l(x) = BN - ReLU - Conv$，也就是先做Batch Normalization，再做ReLU最后做卷积。这里作者没有说明为什么先用激活函数再卷积，但是可能和<a href="https://arxiv.org/abs/1603.05027" target="_blank" rel="noopener">Identity mappings in deep residual networks</a>这篇文章有关系。</p><h3 id="3-4-Pooling-layers"><a href="#3-4-Pooling-layers" class="headerlink" title="3.4 Pooling layers"></a>3.4 Pooling layers</h3><p>在式1中，要把$x_0, x_1, x_2, \dots, x_{l-1}$这些特征连接起来，需要他们有相同的维度。所以说不能直接在Conv后面接池化。为了在网络中使用池化，作者把网络划分成多个dense block。如图3.2.3即为一个denseblock。而在dense block之间，作者提出了transition层，用于做卷积和池化操作。transition层包括了一个Batch Normaliztion，一个1x1卷积和一个2x2 average pooling。</p><p><img alt="图3.4.1 Dense block" data-src="/posts/ef2be802/denseblock.png" class="lazyload"></p><h3 id="3-5-Growth-rate"><a href="#3-5-Growth-rate" class="headerlink" title="3.5 Growth rate"></a>3.5 Growth rate</h3><p>如果说，每个$H_l$输出了k张feature map，那么在第$l$层，总共会有$k_0 + k × (l − 1)$个feature map。作者把这里的$k$定义为增长率，在后文的实现部分，作者在实验部分说明了即使用很小的增长率也可以获得STOA的效果。</p><h3 id="3-6-Bottleneck-layers"><a href="#3-6-Bottleneck-layers" class="headerlink" title="3.6 Bottleneck layers"></a>3.6 Bottleneck layers</h3><p>瓶颈层是在3x3卷积之前加入1x1的卷积，作者发现使用瓶颈层在DenseNet上尤其有效。加入瓶颈层以后复合函数就变成了$H_l(x) = BN-ReLU-Conv(1× 1)-BN-ReLU-Conv(3×3)  $。运用了瓶颈层的DenseNet就叫DenseNet-B。</p><h3 id="3-7-Compression"><a href="#3-7-Compression" class="headerlink" title="3.7 Compression"></a>3.7 Compression</h3><p>这里所谓的压缩，就是在transition层中，减少feature map的数量，从而达到进一步“ensilm”模型的效果。作者定义了压缩因子$\theta$，如果说原本有m个feature map，那么只保留$\lfloor \theta*x \rfloor$个feature map。运用了Compression的DenseNet就叫DenseNet-C，如果即使用了瓶颈层，又用了Compression，就是DenseNet-BC了。  </p><h2 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h2><p>文章后面是一些实验，不再详细说明了。在这篇博客里面，有一些不完美的地方，比如3.3中，先ReLU再Conv的原因、3.4中AP和MP的选择、3.7中compression的实现方法等等。可能作进一步了解以后可以更新本篇。</p><hr><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] 论文阅读 | CVPR2017(Best Paper) | Densely Connected Convolutional Networks, <a href="https://www.jianshu.com/p/cced2e8378bf" target="_blank" rel="noopener">https://www.jianshu.com/p/cced2e8378bf</a>, 2019.3</p><p>[2] DenseNet 论文阅读笔记, <a href="https://www.cnblogs.com/zhhfan/p/10187634.html" target="_blank" rel="noopener">https://www.cnblogs.com/zhhfan/p/10187634.html</a>, 2018.12</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/pdf/1608.06993.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;官方实现和预
      
    
    </summary>
    
    
      <category term="深度学习" scheme="http://blog.logan-qiu.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://blog.logan-qiu.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="理论" scheme="http://blog.logan-qiu.cn/tags/%E7%90%86%E8%AE%BA/"/>
    
      <category term="DenseNet" scheme="http://blog.logan-qiu.cn/tags/DenseNet/"/>
    
      <category term="论文阅读" scheme="http://blog.logan-qiu.cn/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>关于这首柳永的《鹤冲天》</title>
    <link href="http://blog.logan-qiu.cn/posts/da209311/"/>
    <id>http://blog.logan-qiu.cn/posts/da209311/</id>
    <published>2019-12-06T11:40:42.000Z</published>
    <updated>2019-12-07T17:02:14.028Z</updated>
    
    <content type="html"><![CDATA[<div>  <div style="width:50%;margin:0 auto;">    <center>鹤冲天·黄金榜上</center>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;黄金榜上，偶失龙头望。明代暂遗贤，如何向。未遂风云便，争不恣狂荡。何须论得丧？才子词人，自是白衣卿相。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;烟花巷陌，依约丹青屏障。幸有意中人，堪寻访。且恁偎红倚翠，风流事，平生畅。青春都一饷。忍把浮名，换了浅斟低唱。  </div></div>  <hr><p>有人说，柳永的词格调不高。确实不高，单看上阕的牢骚话倒是觉得不错，尤其是“才子词人，自是白衣卿相”，把柳永对自身才气的自信和对落榜$^{[1]}$的不甘展现的淋漓尽致。然读到下阙的时候，格调开始逐渐走低。当读到“青春都一饷”的时候，我期待着作者漂亮的结句来点睛，却等来了一个“忍把浮名”。这就很操蛋了，若是只有“把浮名换了浅斟低唱”已是下乘，加上“忍”字便是落入下下之流。于是整篇词的情感变化变成了：自闭了—&gt;尝试想开—&gt;我又自闭了的过程。有人给这首词打上了豪迈的标签，但是在我看来，所有的豪迈都毁在了“忍”字上。   </p><p>这是柳永的早期作品，大概更能够和年轻人产生共鸣吧，我认为这首词的动人之处，在于真实。你看作者的语言：“如何向”、“争不”、“且恁”，失意了，发发牢骚。但是牢骚发完，该伤感还得伤感，绝不为了格调来一个“一蓑烟雨任平生”。烟花巷陌，是柳永寻欢的场所，他要去找他的意中人，诉说心中的烦闷。但是这还不够，最后还要酸一把、嘴硬一下，<strong>忍把浮名，换了浅斟低唱</strong>。这个毁了全词文学价值的字，是柳永真情的流露，让我们能够感受柳永的内心，体会柳永的性情。也是这个“忍”字出来的时候，我发现了我和柳永性格上的相似，这一刻，我们仿佛成为了相识多年的旧友，这种感觉太美妙了。（颇有小杠精找朋友的感觉）</p><hr><p>[1] 现在好像大部分理解为未中状元，我认为落榜更加合适一些。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;div&gt;
  &lt;div style=&quot;width:50%;margin:0 auto;&quot;&gt;
    &lt;center&gt;鹤冲天·黄金榜上&lt;/center&gt;
    &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;黄金榜上，偶失龙头望。
      
    
    </summary>
    
    
      <category term="杂谈" scheme="http://blog.logan-qiu.cn/categories/%E6%9D%82%E8%B0%88/"/>
    
    
      <category term="诗词" scheme="http://blog.logan-qiu.cn/tags/%E8%AF%97%E8%AF%8D/"/>
    
  </entry>
  
  <entry>
    <title>深度学习之卷积神经网络</title>
    <link href="http://blog.logan-qiu.cn/posts/17556c44/"/>
    <id>http://blog.logan-qiu.cn/posts/17556c44/</id>
    <published>2019-12-05T09:01:49.000Z</published>
    <updated>2019-12-06T03:11:54.829Z</updated>
    
    <content type="html"><![CDATA[<p>卷积神经网络在计算机视觉领域取得了巨大的成功，甚至在音频处理等其他领域，卷积神经网络也能发挥较好的效果。那么，卷积到底是怎么样的一种运算呢？为什么卷积神经网络在图像处理上会有这么大的优势呢？让我们一起深入研究卷积神经网络的原理，学习卷积神经网络，能让你处理远比MNIST数据集复杂的图片，开启视觉计算的新领域。</p><h2 id="什么是卷积？"><a href="#什么是卷积？" class="headerlink" title="什么是卷积？"></a>什么是卷积？</h2><h3 id="二维卷积"><a href="#二维卷积" class="headerlink" title="二维卷积"></a>二维卷积</h3><p>卷积（convolution），其实就是一种数学运算。让我们来看一下面的这个卷积运算的例子：</p><script type="math/tex; mode=display">\left[ \begin{matrix}   1 & 2 & 3 & 1 & 2 & 3  \\   4 & 5 & 6 & 4 & 5 & 6  \\   7 & 8 & 9 &7 & 8 & 9 \\ 1 & 2 & 3 & 1 & 2 & 3 \\   4 & 5 & 6 & 4 & 5 & 6  \\   7 & 8 & 9 &7 & 8 & 9 \end{matrix}  \right] * \left[ \begin{matrix}  1 & 0 & -1 \\ 1 & 0 & -1 \\1 & 0 & -1 \end{matrix}\right] = \left[ \begin{matrix} -6 &3&3&-6\\-6 &3&3&-6\\-6 &3&3&-6\\-6 &3&3&-6\end{matrix}\right] \tag{1}</script><p>事实上，卷积是一种非常简单的运算，与矩阵乘法不同的是，他对矩阵维度要求不是很高，允许不同大小的矩阵进行运算。为了方便说明，我们首先定义一下几个说法。</p><ul><li><p>(1)式中卷积符号左侧的6x6矩阵叫<strong>被卷积矩阵</strong></p></li><li><p>(1)式中卷积符号右侧的3x3矩阵叫做<strong>卷积核</strong></p></li></ul><p>卷积运算的步骤如下：  </p><ol><li><p>把卷积核“覆盖”到被卷积矩阵左上角的位置，将对应元素相乘并相加，作为结果矩阵中的左上角元素。（1）式中的运算过程如下：</p><script type="math/tex; mode=display">1\times1+0\times2+(-1)\times3+1\times4+0\times5+(-1)\times6+1\times7+0\times8+(-1)\times9=-6</script></li><li><p>把卷积核右移一格，重复进行1操作。如果到行尾，则换行继续，直到“覆盖”到右下角元素为止（这一步有点像滑动窗口）。</p></li></ol><p>通过GIF可以形象地说明这一运算的过程：</p><p><img alt data-src="/posts/17556c44/conv.gif" class="lazyload"></p><p><em>不过，我们这里的卷积和数学上讲的卷积有一些区别。数学上的卷积要多一个把被卷积矩阵旋转180度的操作。</em></p><h3 id="三维卷积"><a href="#三维卷积" class="headerlink" title="三维卷积"></a>三维卷积</h3><p>由于图像有可能是彩色的，因此图像组成的矩阵会变成 <code>宽度</code>x<code>高度</code>x<code>通道数量</code>的三阶矩阵。彩色图片的通道数（num of channels）一般为3。然而，三维而计算方法和二维的计算方法是一样的，只是原本的3x3的卷积核变为3x3x3的而已，求和的时候变成27个数相加，最后结果是一个二阶矩阵。</p><h2 id="为什么卷积能用于视觉领域"><a href="#为什么卷积能用于视觉领域" class="headerlink" title="为什么卷积能用于视觉领域"></a>为什么卷积能用于视觉领域</h2><p>利用卷积运算，计算机可以完成<strong>边缘检测</strong>的任务。假如有矩阵如下：</p><script type="math/tex; mode=display">\left[ \begin{matrix} 0&0&1&1&0&0\\0&0&1&1&0&0\\0&0&1&1&0&0\\0&0&1&1&0&0\\0&0&1&1&0&0\\0&0&1&1&0&0 \end{matrix} \right]</script><p>如果用(1)式中的卷积核，对这张图片做卷积，结果会是这样的：</p><script type="math/tex; mode=display">\left[ \begin{matrix} 10&10&10&0&0&0\\10&10&10&0&0&0\\10&10&10&0&0&0\\10&10&10&0&0&0\\10&10&10&0&0&0\\10&10&10&0&0&0 \end{matrix} \right] * \left[ \begin{matrix}  1 & 0 & -1 \\ 1 & 0 & -1 \\1 & 0 & -1 \end{matrix}\right] = \left[ \begin{matrix} 0&30&30&0\\0&30&30&0\\0&30&30&0\\0&30&30&0\end{matrix}\right]\tag{2}</script><p>如果把他们转化成图片，可以很直观的看到，原图左侧是黑色，右侧为白色；而图像经过卷积以后变成中间一条黑色，两边是白色。这说明了原本黑白交接的边界线被卷积识别出来了。因此，(1)(2)中的卷积核实际上是起到了检测图片垂直边缘的作用。同样地，如果给这个卷积核加上一个90度的旋转变换，可以得到一个水平边缘检测器。不难想象，如果在神经网络中使用卷积运算，网络就可以通过拟合卷积核中的参数来检测不同方向上的边缘。这就是卷积提取图像特征的方式。</p><p>事实上，卷积神经网络在一个卷积层中会使用多个卷积核。每一个卷积核进行卷积运算都会得到一个二阶矩阵，这个矩阵我们把他叫做<strong>feature map</strong>(特征图)。使用n个卷积核就会形成n个feature map。最后我们会把这些feature map“叠”在一起，进入下一层计算，下一层的输入通道数即为n。</p><p><img alt data-src="/posts/17556c44/edge_detect.jpg" class="lazyload"></p><h2 id="卷积运算的常用参数"><a href="#卷积运算的常用参数" class="headerlink" title="卷积运算的常用参数"></a>卷积运算的常用参数</h2><h3 id="卷积核大小"><a href="#卷积核大小" class="headerlink" title="卷积核大小"></a>卷积核大小</h3><p>卷积核大小，顾名思义，就是卷积核矩阵的维度。通常为奇数，常见的取值有3x3，5x5和7x7。现在一般认为小卷积核的效果比较好。一个7x7的卷积，其实可以用两个3x3卷积来代替。</p><h3 id="Padding和Stride"><a href="#Padding和Stride" class="headerlink" title="Padding和Stride"></a>Padding和Stride</h3><p>padding（填充的层数）: 我们从(1)的例子中可以看到，做完卷积运算以后，图片变小了（从6x6变成了4x4）。这不利于进一步提取图像特征。因此，我们可以使用padding来填充图像外围区域，比如填0，来把原图变大，这样可以保证卷积后图片的大小。</p><p>stride（步长）：指的是在“滑动窗口”的过程中，每次滑动的步长。你可以对横向和纵向分别设置。</p><h2 id="卷积的好伙伴——池化（Pooling）"><a href="#卷积的好伙伴——池化（Pooling）" class="headerlink" title="卷积的好伙伴——池化（Pooling）"></a>卷积的好伙伴——池化（Pooling）</h2><p>观察(2)中的卷积结果，可以发现用卷积做边缘检测时，边界线被“弄粗”了。这个时候我们可以用池化来解决这个问题。池化也是一种非常简单的数学运算，我们还是先来看一个例子：</p><script type="math/tex; mode=display">\left[ \begin{matrix} 0&0&30&30&0&0\\0&0&30&30&0&0\\0&0&30&30&0&0\\0&0&30&30&0&0\\0&0&30&30&0&0\\0&0&30&30&0&0 \end{matrix} \right]\overset{2\times2MP,s=2}\longrightarrow \left[ \begin{matrix} 0&30&0\\0&30&0\\0&30&0 \end{matrix} \right] \tag{3}</script><p>还是那卷积运算中的滑动窗口的眼光来看，这里用了2x2池化，也就是用2x2的窗口去套原矩阵，并且选择2x2窗口中的最大值，组成新的矩阵。你也可以选择2x2窗口中的平均值，这是两种不同的池化方式，一个叫Max pooling（最大池化），另一个叫Average pooling（平均池化）。我们一般用缩写MP和AP来表示。  </p><p>池化还有一个好处，他可以把feature map缩小，减少运算数量。同时，池化是不需要参数的，也就是不参与训练，不会耗费训练时间。</p><h2 id="卷积神经网络（CNN）"><a href="#卷积神经网络（CNN）" class="headerlink" title="卷积神经网络（CNN）"></a>卷积神经网络（CNN）</h2><p>卷积神经网络由输入层，卷积层和全连接层构成。</p><h3 id="卷积层（Conv）"><a href="#卷积层（Conv）" class="headerlink" title="卷积层（Conv）"></a>卷积层（Conv）</h3><p>卷积层就是把图像对多个卷积核进行卷积，然后再对得到的feature map做池化处理。<strong>一个卷积层通常由卷积+池化组成</strong>（大概是池化没有参数，所有不配单独成池化层233333）。  </p><p>在卷积层中，出了刚刚提到的卷积参数以外，还有一个超参数需要配置，就是卷积核的个数。这个在上文也已有介绍。</p><h3 id="全连接层（FC）"><a href="#全连接层（FC）" class="headerlink" title="全连接层（FC）"></a>全连接层（FC）</h3><p>一个CNN网络通常会包含多个卷积层，然后在后面接一个或者多个全连接层。全连接层实际上就是以前接触过的普通的神经网络中的层。负责把conv层提取到的特征进行归纳计算。</p><h3 id="卷积层的优势"><a href="#卷积层的优势" class="headerlink" title="卷积层的优势"></a>卷积层的优势</h3><ol><li>卷积运算量不大，可以高效提取特征信息。</li><li>池化层可以大大减少运算量，加快训练速度。</li><li>多个卷积层可以把图片中的参数变少，比如128x128x3的图片，经过数轮卷积层以后变成7x7x128的feature map，图片参数从49152减小到6272。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;卷积神经网络在计算机视觉领域取得了巨大的成功，甚至在音频处理等其他领域，卷积神经网络也能发挥较好的效果。那么，卷积到底是怎么样的一种运算呢？为什么卷积神经网络在图像处理上会有这么大的优势呢？让我们一起深入研究卷积神经网络的原理，学习卷积神经网络，能让你处理远比MNIST数据
      
    
    </summary>
    
    
      <category term="深度学习" scheme="http://blog.logan-qiu.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://blog.logan-qiu.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="理论" scheme="http://blog.logan-qiu.cn/tags/%E7%90%86%E8%AE%BA/"/>
    
      <category term="CNN" scheme="http://blog.logan-qiu.cn/tags/CNN/"/>
    
      <category term="卷积神经网络" scheme="http://blog.logan-qiu.cn/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Hello hexo+Butterfly</title>
    <link href="http://blog.logan-qiu.cn/posts/8ab1f019/"/>
    <id>http://blog.logan-qiu.cn/posts/8ab1f019/</id>
    <published>2019-12-03T13:23:35.000Z</published>
    <updated>2019-12-05T13:54:37.331Z</updated>
    
    <content type="html"><![CDATA[<p>我原是想自己写一个完整的博客前后端，现在看身边的小伙伴都开始交换友链了，再想想即将到来的期末考试，还有自己拖下的一些项目，还是算了吧。只能这样安慰自己：明明已经有这么多现成的开源项目了，何必重复劳动，手撸轮子。既然是自己的博客，可以多发表发表自己的观点，大概不会像用博客园那样有拘束感。</p><p>第一篇就讲一讲搭建Hexo时遇到的一些问题吧。</p><h2 id="Hexo的安装"><a href="#Hexo的安装" class="headerlink" title="Hexo的安装"></a>Hexo的安装</h2><p>如果是使用Hexo+GitxxPage的话，无需把Hexo安装在服务器上，只要安装在本机上即可。因为到时候使用的是gitxx的服务器，本机直接<code>deploy</code>比较方便。</p><p>首先是安装一些依赖环境，git啦，node啦，这些都不细说。主要是讲用npm装hexo本体的时候的这行命令。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">npm install hexo-cli -g</span></pre></td></tr></table></figure><p>其中的install在有的教程里面用的是缩写i，这是等价的。后面的 <code>-g</code> 参数意味着全局安装。也就是说在任意目录下，你都可以运行hexo。</p><h3 id="MacOS系统中的问题"><a href="#MacOS系统中的问题" class="headerlink" title="MacOS系统中的问题"></a>MacOS系统中的问题</h3><p>mac通过npm安装的时候可能会报一堆权限错误的问题，即使加了<code>sudo</code>，也有可能造成安装卡住不动的问题。这是由于访问<code>/usr/local</code>目录权限不足造成的，这时候就要使用chown来解决。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">sudo chown R $USER /usr/local</span></pre></td></tr></table></figure><h3 id="Linux系统中的问题"><a href="#Linux系统中的问题" class="headerlink" title="Linux系统中的问题"></a>Linux系统中的问题</h3><p>我装nodejs的时候是直接下载二进制包解压，然后配置软链接的。所以全局安装的包有可能不在环境变量下，还需要再把<code>node/bin</code>放入环境变量中，才能生效。Windows版本可能也有环境变量的问题，需要注意。</p><h2 id="配置SSH密钥连接GitHub账户"><a href="#配置SSH密钥连接GitHub账户" class="headerlink" title="配置SSH密钥连接GitHub账户"></a>配置SSH密钥连接GitHub账户</h2><p>一开始配ssh密钥的时候github一直提示，密钥格式错误，其实是复制错了文件。生成文件的时候他可能会告诉你生成的是<code>~/.ssh/id_rsa</code>，其实要复制的是<code>~/.ssh/id_rsa.pub</code>文件。同时还需要注意的是要使用cat命令而不是vim命令打开文件，用vim复制可能会包含多余的字符。</p><h2 id="Butterfly主题的使用"><a href="#Butterfly主题的使用" class="headerlink" title="Butterfly主题的使用"></a>Butterfly主题的使用</h2><p>跟着官方文档一步一步走就行了。复制配置文件butterfly.yml那一步可能会让人感到困惑。被复制的文件应该是<code>themes/Butterfly/_config.yml</code>，而不是博客文件根目录下的那个配置文件。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;我原是想自己写一个完整的博客前后端，现在看身边的小伙伴都开始交换友链了，再想想即将到来的期末考试，还有自己拖下的一些项目，还是算了吧。只能这样安慰自己：明明已经有这么多现成的开源项目了，何必重复劳动，手撸轮子。既然是自己的博客，可以多发表发表自己的观点，大概不会像用博客园那
      
    
    </summary>
    
    
      <category term="杂谈" scheme="http://blog.logan-qiu.cn/categories/%E6%9D%82%E8%B0%88/"/>
    
    
      <category term="Hexo" scheme="http://blog.logan-qiu.cn/tags/Hexo/"/>
    
      <category term="Butterfly" scheme="http://blog.logan-qiu.cn/tags/Butterfly/"/>
    
  </entry>
  
</feed>
