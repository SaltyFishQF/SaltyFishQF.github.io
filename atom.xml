<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>QF的个人博客</title>
  
  <subtitle>忍把浮名,换了浅斟低唱</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://blog.logan-qiu.cn/"/>
  <updated>2019-12-17T12:19:52.538Z</updated>
  <id>http://blog.logan-qiu.cn/</id>
  
  <author>
    <name>QF</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>编译原理复习整理（三）</title>
    <link href="http://blog.logan-qiu.cn/posts/c9ecfa8b/"/>
    <id>http://blog.logan-qiu.cn/posts/c9ecfa8b/</id>
    <published>2019-12-13T09:32:00.000Z</published>
    <updated>2019-12-17T12:19:52.538Z</updated>
    
    <content type="html"><![CDATA[<p>参考课程为自哈尔滨工业大学的 <a href="https://www.icourse163.org/learn/HIT-1002123007#/learn/announce" target="_blank" rel="noopener">《编译原理》</a>，参考教材为《编译原理及实践教程》（第三版）。</p><p>本文划分仅考虑本人每日复习量，没有章节划分参考价值。</p><h2 id="CFG的分析树"><a href="#CFG的分析树" class="headerlink" title="CFG的分析树"></a>CFG的分析树</h2><p><img alt="CFG的分析树" data-src="/posts/c9ecfa8b/cfgfxs.png" class="lazyload"></p><p>根节点对应文法开始符号。  </p><p>内部节点表示对一个产生式的应用，该节点的标号是此产生式的左部。该节点的子节点的标号从左到右构成了产生式的右部。  </p><p>叶节点的标号既可以是非终结符，也可以是终结符。从左到右排列叶节点得到的符号串称为这棵树的<strong>产出</strong>或者<strong>边缘</strong>。</p><p>分析树是推导的图形化表示</p><p>给定一个推导$S\Rightarrow a_1\Rightarrow a_2 \Rightarrow \dots \Rightarrow a_n$，对于推导过程中的每一个句型$a_i$，都可以构造出一个边缘为$a_i$的分析树。</p><h3 id="（句型的）短语"><a href="#（句型的）短语" class="headerlink" title="（句型的）短语"></a>（句型的）短语</h3><p>给定一个句型，其分析树中的每一棵<strong>子树的边缘</strong>称为该句型的短语。  </p><p>如果子树只有父子两代节点，那么这棵子树的边缘称为该句型的一个<strong>直接短语</strong>。</p><p><img alt="短语" data-src="/posts/c9ecfa8b/dy.png" class="lazyload"></p><h2 id="自底向上的语法分析"><a href="#自底向上的语法分析" class="headerlink" title="自底向上的语法分析"></a>自底向上的语法分析</h2><p>从分析树的底部（叶节点）向顶部（根节点）方向构造分析树。可以看成是输入串$w$规约为文法开始符号$S$的过程。  </p><p>自底向上的语法分析采用最左规约方式（反向构造最右推导）。自底向上语法分析的通用框架是<strong>移入-规约分析</strong>。</p><h3 id="移入-规约分析"><a href="#移入-规约分析" class="headerlink" title="移入-规约分析"></a>移入-规约分析</h3><p><img alt="移入-规约分析" data-src="/posts/c9ecfa8b/yrgyfx.png" class="lazyload"></p><ul><li>每次规约的符号串称为<strong>句柄</strong>。</li><li>站内符号串+剩余输入=<strong>规范句型</strong></li></ul><h3 id="移入-规约分析中存在的问题"><a href="#移入-规约分析中存在的问题" class="headerlink" title="移入-规约分析中存在的问题"></a>移入-规约分析中存在的问题</h3><p>句型实际上是当前句型的最左直接短语。</p><p><img alt data-src="/posts/c9ecfa8b/zzzjdy.png" class="lazyload"></p><h2 id="算符优先分析"><a href="#算符优先分析" class="headerlink" title="算符优先分析"></a>算符优先分析</h2><h3 id="算符文法"><a href="#算符文法" class="headerlink" title="算符文法"></a>算符文法</h3><p>一个文法，如果它的任何产生式的右部都不含两个相继（并列）的非终结符结构，即不含如下形式的产生式：  </p><script type="math/tex; mode=display">P \rightarrow \dots QR\dots</script><p>则称该文法是<strong>算符文法</strong>。</p><p>假定$G$是不含$ε$产生式的算符文法，终结符对a、b之间的优先关系定义如下。</p><ul><li>$a \dot= b$当且仅当文法G中含有形如$P \rightarrow ab$或者$P \rightarrow aQb$的产生式。</li><li>$a \dot&lt; b$当且仅当文法G中含有形如$P \rightarrow aR$的产生式，且$R\Rightarrow^+b\dots$或$R\Rightarrow^+Qb\dots$。</li><li>$a \dot&gt; b$当且仅当文法G中含有形如$P \rightarrow Rb$的产生式，且$R\Rightarrow^+a\dots$或$R\Rightarrow^+aQ\dots$。</li></ul><p><strong>算符优先文法</strong>：若G是一个不含空产生式的算符文法，任何终结符对(a, b)只满足下述三种优先关系之一：</p><script type="math/tex; mode=display">a \dot= b, a \dot< b, a \dot> b</script><p>则称G为一个算符优先文法，简称OPG文法。算符优先文法是无二义性的。</p><h3 id="算符优先表"><a href="#算符优先表" class="headerlink" title="算符优先表"></a>算符优先表</h3><h4 id="FirstVT和LastVT集"><a href="#FirstVT和LastVT集" class="headerlink" title="FirstVT和LastVT集"></a>FirstVT和LastVT集</h4><p>对于文法G的任一非终结符P，定义两个集合：</p><p>FirstVT(P)：P能推倒的第一个终结符构成的集合。</p><p>LastVT(P)：P能推导的最后一个终结符构成的集合。</p><h4 id="算符优先表的构造"><a href="#算符优先表的构造" class="headerlink" title="算符优先表的构造"></a>算符优先表的构造</h4><ul><li>$\dot=$关系：若有形如$P \rightarrow \dots ab\dots$或$P \rightarrow \dots aQb\dots$的产生式，则$a\dot= b$。可以直接看产生式得到。</li><li>$\dot&lt;$关系：若有形如$Q\rightarrow\dots aP\dots$的产生式，对任何$b\in FirstVT(P)$，有$a\dot&lt;b$。</li><li>$\dot&gt;$关系：若有形如$Q\rightarrow\dots Pb\dots$的产生式，对任何$a\in LastVT(P)$，有$a\dot&gt;b$。</li></ul><h3 id="算符优先分析过程"><a href="#算符优先分析过程" class="headerlink" title="算符优先分析过程"></a>算符优先分析过程</h3><p>和移进-规约过程基本相同。只是在满足$\dot&lt;$或者$\dot=$时移入，在$\dot&gt;$时规约。  </p><p>算符优先分析中，句型可以用一般形式(N)表示，在规约过程中，只需终结符相同，非终结符位置相同即可。</p><h2 id="LR分析法"><a href="#LR分析法" class="headerlink" title="LR分析法"></a>LR分析法</h2><h3 id="LR文法"><a href="#LR文法" class="headerlink" title="LR文法"></a>LR文法</h3><p>可以运用LR分析法的文法称为LR文法。LR文法是最大的、可以构造出相应<strong>移入-规约语法分析器</strong>的文法类。  </p><ul><li>L：对输入进行从左到右的扫描</li><li>R：反向构造出一个最右推导序列</li></ul><p>LR(k)分析：需要向前查看k个输入符号的LR分析。当k=0和k=1这两种情况时具有实际意义。当省略(k)时，表示k=1。  </p><h3 id="LR分析法的基本原理"><a href="#LR分析法的基本原理" class="headerlink" title="LR分析法的基本原理"></a>LR分析法的基本原理</h3><p>句柄是逐步形成的，用“状态”表示句柄识别的进展程度。</p><p><img alt data-src="/posts/c9ecfa8b/jbdzt.png" class="lazyload"></p><h3 id="LR分析器（自动机）的总体结构"><a href="#LR分析器（自动机）的总体结构" class="headerlink" title="LR分析器（自动机）的总体结构"></a>LR分析器（自动机）的总体结构</h3><p><img alt data-src="/posts/c9ecfa8b/lrfxq.png" class="lazyload"></p><p>Ex: 文法$S\rightarrow BB \\ B\rightarrow aB \\ B\rightarrow b$对应的LR分析表为：</p><p><img alt data-src="/posts/c9ecfa8b/lrgxb1.png" class="lazyload"></p><p>在上表中，每一行对应一个状态，一共有7个状态；Action表中的每一列分别对应文法中的终结符，以及结束符号#；Action表中的每一项表示对应的语法分析动作，sn表示将当前符号移入到分析栈中，并且进入状态n。rn表示用第n个产生式进行规约；GOTO表的每一列对应着文法中的非终结符；GOTO表中的每一项表示在某一状态遇到某一非终结符遇到的后继状态。  </p><p>假设输入为b a b，则会有以下的分析过程：</p><p><img alt data-src="/posts/c9ecfa8b/fxgc1.png" class="lazyload"></p><p><img alt data-src="/posts/c9ecfa8b/fxgc2.png" class="lazyload"></p><p><img alt data-src="/posts/c9ecfa8b/fxgc3.png" class="lazyload"></p><p><img alt data-src="/posts/c9ecfa8b/fxgc4.png" class="lazyload"></p><p><img alt data-src="/posts/c9ecfa8b/fxgc5.png" class="lazyload"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;参考课程为自哈尔滨工业大学的 &lt;a href=&quot;https://www.icourse163.org/learn/HIT-1002123007#/learn/announce&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;《编译原理》&lt;/a&gt;，参考教材为《
      
    
    </summary>
    
    
      <category term="编译原理" scheme="http://blog.logan-qiu.cn/categories/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/"/>
    
    
      <category term="编译原理" scheme="http://blog.logan-qiu.cn/tags/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/"/>
    
      <category term="期末复习" scheme="http://blog.logan-qiu.cn/tags/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>编译原理复习整理（二）</title>
    <link href="http://blog.logan-qiu.cn/posts/84f259d9/"/>
    <id>http://blog.logan-qiu.cn/posts/84f259d9/</id>
    <published>2019-12-12T05:52:57.000Z</published>
    <updated>2019-12-12T14:28:21.624Z</updated>
    
    <content type="html"><![CDATA[<p>参考课程为自哈尔滨工业大学的 <a href="https://www.icourse163.org/learn/HIT-1002123007#/learn/announce" target="_blank" rel="noopener">《编译原理》</a>，参考教材为《编译原理及实践教程》（第三版）。</p><p>本文划分仅考虑本人每日复习量，没有章节划分参考价值。</p><h2 id="文法的定义-Grammer"><a href="#文法的定义-Grammer" class="headerlink" title="文法的定义(Grammer)"></a>文法的定义(Grammer)</h2><p>文法是描述语言的语法结构的形式规则（即语法规则）。</p><p>以自然语言为例：</p><ul><li><p>&lt;句子&gt; $\rightarrow$ &lt;名词短语&gt;&lt;动词短语&gt;</p></li><li><p>&lt;名词短语&gt; $\rightarrow$ &lt;形容词&gt; &lt;名词短语&gt; </p></li><li>&lt;名词短语&gt; $\rightarrow$ &lt;名词&gt;</li><li><p>&lt;动词短语&gt; $\rightarrow$ &lt;动词&gt; &lt;名词短语&gt;</p></li><li><p>&lt;形容词&gt; $\rightarrow$ little </p></li><li>&lt;名词&gt; $\rightarrow$ boy </li><li>&lt;名词&gt; $\rightarrow$ apple </li><li>&lt;动词&gt; $\rightarrow$ eat</li></ul><p>用尖括号括起来的的符号，是<strong>非终结符</strong>，他们不会出现在最终的句子当中。出现在最终的句子符号是<strong>终结符</strong>，他们由不带尖括号的符号。</p><p><strong>产生式</strong>：按照一定规则书写的、用于定义语法范畴的规则，说明了终结符和非终结符组成符号串的方式。形如$a\rightarrow b$，其中$a$为左部，$a\in V^+$，且至少包含一个非终结符。$b$为右部，$b\in V^*$。</p><h3 id="文法的形式化定义"><a href="#文法的形式化定义" class="headerlink" title="文法的形式化定义"></a>文法的形式化定义</h3><p>$G = (V_T, V_N,P,S)$</p><p>$V_T$：终结符集合</p><p>$V_N$：非终结符集合</p><p>$P$：产生式集合</p><p>$S$：开始符号，表示该文法中最大的语法成分。</p><p>约定：在不引起歧义的前提下，可以只写文法的产生式。</p><p><img alt="只写文法的产生式" data-src="/posts/84f259d9/css.png" class="lazyload"></p><h3 id="产生式的简写"><a href="#产生式的简写" class="headerlink" title="产生式的简写"></a>产生式的简写</h3><p>对一组有相同左部的$\alpha$产生式 $\alpha\rightarrow \beta_1, \alpha\rightarrow \beta_n, \dots, \alpha\rightarrow \beta_n$   </p><p>可以简写为： $\alpha\rightarrow \beta_1| \beta_2|\dots | \beta_n$</p><p><img alt="产生式的简写" data-src="/posts/84f259d9/cssdjx.png" class="lazyload"></p><h3 id="符号约定"><a href="#符号约定" class="headerlink" title="符号约定"></a>符号约定</h3><ul><li><p>下述符号是终结符</p><ol><li>字母表中排在前面的小写字母，如 a、b、c</li><li>运算符，如 +、 *等</li><li>标点符号，如括号、逗号等</li><li>数字0、1、. . . 、9</li><li>粗体字符串，如<strong>id</strong>、<strong>if</strong>等</li></ol></li><li><p>下述符号是非终结符</p><ol><li>字母表中排在前面的大写字母，如A、B、 C</li><li>字母S。通常表示开始符号</li><li>小写、斜体的名字，如 <em>expr</em>、<em>stmt</em>等</li><li>代表程序构造的大写字母。如E(表达式)、T(项) 和F(因子)</li></ol></li><li>字母表中排在后面的大写字母（如X、Y、Z） 表示文法符号（即终结符或非终结符）</li><li>字母表中排在后面的小写字母（主要是u、v、. . . 、z） 表示终结符号串（包括空串）</li><li>小写希腊字母，如α、β、γ，表示文法符号串（包括空串）</li><li>除非特别说明，第一个产生式的左部就是开始符号</li></ul><p><img alt="总结" data-src="/posts/84f259d9/fhyd.png" class="lazyload"></p><h2 id="语言的定义"><a href="#语言的定义" class="headerlink" title="语言的定义"></a>语言的定义</h2><h3 id="推倒和规约"><a href="#推倒和规约" class="headerlink" title="推倒和规约"></a>推倒和规约</h3><p>给定文法$G = (V_T, V_N,P,S)$， 如果$\alpha\rightarrow\beta\in P$，那么可以将符号串$\gamma\alpha\delta$中的$α$<strong>替换</strong>为$β$，也就是说，将$γαδ$ <strong>重写</strong>(rewrite)为$γβδ$，记作 $γαδ \rightarrow γβδ$。此时，称文法中的符号串$ γαδ $<strong>直接推导</strong>(directly derive)出 $γβδ$ 。</p><p>简而言之，就是用产生式的右部替换产生式的左部。</p><h2 id="自顶向下分析（Top-Down-Parsing）"><a href="#自顶向下分析（Top-Down-Parsing）" class="headerlink" title="自顶向下分析（Top-Down Parsing）"></a>自顶向下分析（Top-Down Parsing）</h2><p>从分析树的顶部（根节点）向底部（叶节点）方向构造分析树，可以看成是从文法的开始符号$S$推倒出词串$w$的过程。</p><p><img alt="自顶向下的分析" data-src="/posts/84f259d9/tdp.png" class="lazyload"></p><p>每一步推倒中都要做出两个选择:</p><ul><li>替换当前句型中的<strong>哪个非终结符</strong></li><li>用该非终结符的<strong>哪个候选式</strong>进行替换</li></ul><h3 id="最左推倒-Left-most-Derivation"><a href="#最左推倒-Left-most-Derivation" class="headerlink" title="最左推倒(Left-most Derivation)"></a>最左推倒(Left-most Derivation)</h3><p>在最左推倒中，总是选择每个句型的<strong>最左非终结符</strong>进行替换</p><p><img alt="最左推倒" data-src="/posts/84f259d9/lmd.png" class="lazyload"></p><p>最右规约是最左推倒的逆过程。  </p><p>如果$S\Rightarrow^{<em>} _{lm}a$，则称$a$是当前文法的<em>*最左句型</em></em></p><h3 id="最右推倒-Right-most-Derivation"><a href="#最右推倒-Right-most-Derivation" class="headerlink" title="最右推倒(Right-most Derivation)"></a>最右推倒(Right-most Derivation)</h3><p>在最右推倒中，总是选择每个句型的<strong>最右非终结符</strong>进行替换</p><p><img alt="最右推倒" data-src="/posts/84f259d9/rmd.png" class="lazyload"></p><p>在自底向上的分析中，总是采用最左归约的方式，因此把<strong>最左归约</strong>称为<strong>规范归约</strong>，而<strong>最右推导</strong>相应地称为<strong>规范推导</strong></p><h3 id="最左推倒和最右推倒的唯一性"><a href="#最左推倒和最右推倒的唯一性" class="headerlink" title="最左推倒和最右推倒的唯一性"></a>最左推倒和最右推倒的唯一性</h3><p>给定一棵分析树，在推倒的过程中可以选择任意一个非终结符进行替换。但是最左推倒和最右推倒是唯一的，因为最左/右非终结符是唯一的。</p><p><img alt="最左推倒和最右推倒的唯一性" data-src="/posts/84f259d9/wyx.png" class="lazyload"></p><p>自顶向下的语法分析采用<strong>最左推导</strong>方式,总是选择每个句型的最左非终结符进行替换。根据输入流中的下一个终结符，选择最左非终结符的一个候选式。</p><h2 id="文法转换"><a href="#文法转换" class="headerlink" title="文法转换"></a>文法转换</h2><p>并不是所有的文法都适合自顶向下的分析， 例如  </p><h3 id="问题1-回溯"><a href="#问题1-回溯" class="headerlink" title="问题1 回溯"></a>问题1 回溯</h3><ul><li><p>文法G</p><p>$S \rightarrow aAd|aBe$</p><p>$A\rightarrow c$</p><p>$B\rightarrow e$</p></li><li><p>输入: $a\ b\ c $</p></li></ul><p>输入从$a$开始，而$S$的两个候选式都是以$a$开头的，导致回溯现象。  </p><h3 id="问题2-左递归"><a href="#问题2-左递归" class="headerlink" title="问题2 左递归"></a>问题2 左递归</h3><ul><li><p>文法G</p><p>$E \rightarrow E+T|E-T|T$</p><p>$T \rightarrow T*F|T/F|F$</p><p>$F \rightarrow (E)|id$</p></li><li><p>输入：id + id * id</p></li></ul><p>输入从id开始，需要逐个尝试，陷入$E \rightarrow\ E+T$递归。</p><p>含有 $A\rightarrow Aα$形式产生式的文法称为是直接左递 的 (immediate left recursive )</p><p>如果一个文法中有一个非终结符$A$使得对某个串 $α$ 存 在一个推导$A \Rightarrow ^+Aα$ ，那么这个文法就是左递归的</p><p>经过两步或两步以上推导产生的左递归称为是间接左递归的。</p><h3 id="消除直接左递归"><a href="#消除直接左递归" class="headerlink" title="消除直接左递归"></a>消除直接左递归</h3><p>直接用$A\rightarrow A\alpha|\beta$推倒出来的式子代换即可。</p><p><img alt="消除直接左递归" data-src="/posts/84f259d9/xczjzdg.png" class="lazyload"></p><p>消除左递归付出的代价：引进引进了一些非终结符和$ε$产生式</p><h3 id="消除间接左递归"><a href="#消除间接左递归" class="headerlink" title="消除间接左递归"></a>消除间接左递归</h3><p>用代入法转换成直接左递归消除。</p><p><img alt="消除间接左递归" data-src="/posts/84f259d9/xcjjzdg.png" class="lazyload"></p><h3 id="回溯问题的解决——提取左公因子"><a href="#回溯问题的解决——提取左公因子" class="headerlink" title="回溯问题的解决——提取左公因子"></a>回溯问题的解决——提取左公因子</h3><p><img alt="提取左公因子" data-src="/posts/84f259d9/tqzgyz.png" class="lazyload"></p><h2 id="LL-1-文法"><a href="#LL-1-文法" class="headerlink" title="LL(1)文法"></a>LL(1)文法</h2><h3 id="串首终结符集（FIRST集）"><a href="#串首终结符集（FIRST集）" class="headerlink" title="串首终结符集（FIRST集）"></a>串首终结符集（FIRST集）</h3><p>串首终结符：串首第一个符号，并且是终结符。简称首终结符。</p><p>给定一个文法符号串$α$， $α$的串首终结符集$FIRST(α)$被定义可以从$α$推导出的所有串首终结符构成的集合。如果$α \Rightarrow^* ε$， 那么$ε\in FIRST(α)$中。  </p><h4 id="符号的FIRST集的计算"><a href="#符号的FIRST集的计算" class="headerlink" title="符号的FIRST集的计算"></a>符号的FIRST集的计算</h4><ol><li>把首终结符和空串加入到FIRST集中</li><li>若产生式右部以非终结符打头，那么这个非终结符的FIRST集中的所有终结符，都是产生式左边非终结符能推倒出的首终结符。</li></ol><p><img alt="First集的计算" data-src="/posts/84f259d9/calfirst.png" class="lazyload"></p><h4 id="串的FIRST集的计算"><a href="#串的FIRST集的计算" class="headerlink" title="串的FIRST集的计算"></a>串的FIRST集的计算</h4><ol><li>向$FIRST(X_1,X_2,\dots,X_n)$加入$FIRST(X_1)$中的所有非$ε$符号。</li><li>如果$ε \in FIRST(X_1)$，再加入$FIRST(X_2)$中的所有非$ε$符号，以此类推。</li><li>如果对于所有的$i$，$ε$都在$FIRST(X_i)$中，那么将$ε$加入到$FIRST(X_1,X_2,\dots,X_n)$中。</li></ol><h3 id="FOLLOW集的计算"><a href="#FOLLOW集的计算" class="headerlink" title="FOLLOW集的计算"></a>FOLLOW集的计算</h3><p>$FOLLOW(A)$：可能在某个句型中紧跟在$A$后面的终结符的集合。</p><ol><li>将结束符$#$加入到$FOLLOW(S)$中，S为开始符号。</li><li>如果存在产生式$A\rightarrow \alpha B\beta$，那么$FIRST(\beta)$中的除了$ε$之外的所有符号都在$FOLLOW(B)$中。</li><li>如果存在一个产生式$A\rightarrow \alpha B$，或存在产生式$A\rightarrow \alpha B\beta$，且$FIRST(\beta)$包含$ε$，那么$FOLLOW(A)$中的所有符号都在$FOLLOW(B)$中。</li></ol><p><img alt="FOLLOW集的计算" data-src="/posts/84f259d9/calfollow.png" class="lazyload"></p><h3 id="LL-1-文法-1"><a href="#LL-1-文法-1" class="headerlink" title="LL(1)文法"></a>LL(1)文法</h3><p>若文法$G$是LL(1)的，当且仅当G的任意两个具有相同左部的产生式A → α | β 满足下面的条件： </p><ul><li>不存在终结符a使得α 和β都能够推导出以a开头的串</li><li>$α$和$β$至多有一个能推导出$ε$如果 $β \Rightarrow^*ε$，则$FIRST (α)∩FOLLOW(A) = Φ$； </li><li>如果$α \Rightarrow^*\ ε$，则$FIRST (β)∩FOLLOW(A) =Φ$；</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;参考课程为自哈尔滨工业大学的 &lt;a href=&quot;https://www.icourse163.org/learn/HIT-1002123007#/learn/announce&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;《编译原理》&lt;/a&gt;，参考教材为《
      
    
    </summary>
    
    
      <category term="编译原理" scheme="http://blog.logan-qiu.cn/categories/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/"/>
    
    
      <category term="编译原理" scheme="http://blog.logan-qiu.cn/tags/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/"/>
    
      <category term="期末复习" scheme="http://blog.logan-qiu.cn/tags/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>编译原理复习整理（一）</title>
    <link href="http://blog.logan-qiu.cn/posts/b4e4b501/"/>
    <id>http://blog.logan-qiu.cn/posts/b4e4b501/</id>
    <published>2019-12-11T13:47:36.000Z</published>
    <updated>2019-12-12T14:28:22.366Z</updated>
    
    <content type="html"><![CDATA[<p>参考课程为自哈尔滨工业大学的 <a href="https://www.icourse163.org/learn/HIT-1002123007#/learn/announce" target="_blank" rel="noopener">《编译原理》</a>，参考教材为《编译原理及实践教程》（第三版）。</p><p>本文划分仅考虑本人每日复习量，没有章节划分参考价值。</p><h2 id="字母表"><a href="#字母表" class="headerlink" title="字母表"></a>字母表</h2><p>字母表$\Sigma$十一个有穷符号集合。 （符号包括字母、数字、标点符号）  </p><p>二进制字母表$\{0,1\}$，ASCII字符集，Unicode字符集都属于字母表。</p><h3 id="字母表的运算"><a href="#字母表的运算" class="headerlink" title="字母表的运算"></a>字母表的运算</h3><ul><li>乘积： $\Sigma_1\Sigma_2=\{a,b|a\in\Sigma_1, b\in\Sigma_2\}$  Ex: $\{0,1\}\{a,b\}=\{0a,0b,1a,1b\}$</li><li><p>幂：$\begin{equation}\left\{\begin{array}{lr}\Sigma^0 = \{\epsilon\}(空串) &amp; \\ \Sigma^n=\Sigma^{n-1}\Sigma\end{array}\right.\end{equation}$  Ex: $\{0,1\}^3 = \{000,001,010,011,100,101,110,111\}$</p></li><li><p>正闭包：$\Sigma^+=\Sigma\bigcup\Sigma^2\bigcup\Sigma^3\bigcup\dots$   Ex: $\{a,b,c,d\}^+=\{a,b,c,d,aa,ab,ac,ad,ba,bb,bc,bd,ca,cb,cc,cd,da,db,dc,dd,aaa,\dots\}$</p></li><li><p>（克林）闭包：$\Sigma^*=\Sigma^0\bigcup\Sigma^+$   就是在正闭包的基础上加上一个空串。</p></li></ul><h2 id="串"><a href="#串" class="headerlink" title="串"></a>串</h2><p>设$\Sigma$是一个字母表，$\forall{x\in\Sigma^*}$，称$x$为$\Sigma$上的一个串，串是字母表中符号的一个有穷序列。  </p><p>串$s$的长度通常记为$|s|$，指$s$中符号的个数。   Ex: $|aab|=3$  </p><h3 id="串上的运算"><a href="#串上的运算" class="headerlink" title="串上的运算"></a>串上的运算</h3><ul><li><p>连接：$xy$ ， $x=dog, y=house, xy=doghouse$</p><p>对于空串：$\epsilon s=s\epsilon = s$  </p><p>设$x,y,z$是三个字符串，若$x = yz$，则$y$为$x$的前缀，$z$为$x$的后缀。</p></li><li><p>幂运算：$\begin{equation}\left\{\begin{array}{lr}s^0 = \epsilon, &amp; \\ s^n=s^{n-1}s, &amp;n\geq1\end{array}\right.\end{equation}$    Ex: $s=ba,s^1=ba,s^2=baba,s^3=bababa$</p></li></ul><h2 id="正则表达式-RE"><a href="#正则表达式-RE" class="headerlink" title="正则表达式(RE)"></a>正则表达式(RE)</h2><p>语言可以用集合表示，设语言<script type="math/tex">L = \{a\}\{a,b\}^*(\{\epsilon\}\bigcup(\{.,\_\}\{a,b\}\{a,b\}^*))</script></p><p>正则表达式是一种用来描述正则语言的更紧凑的表示方法。  </p><p>例如<script type="math/tex">r=a(a|b)^*(\epsilon|(.|\_)(a|b)(a|b)^*)</script></p><p>正则表达式可以由较小的正则表达式按照特定规则递归的构建。每个正则表达式$r$定义（表示）一个语言，记为$L(r)$。这个语言是根据$r$的字表达式递归定义的。</p><h3 id="正则表达式的定义"><a href="#正则表达式的定义" class="headerlink" title="正则表达式的定义"></a>正则表达式的定义</h3><ul><li>$\epsilon$是一个RE，$L(\epsilon) = \epsilon$</li><li>如果$a\in\Sigma$，则$a$是一个RE，$L(a) = \{a\}$</li><li>假设$r$和$s$都是RE，表示的语言分别为$L(r)$和$L(s)$，则：<ul><li>$r|s$是一个RE，$L(r|s)=L(r)\bigcup L(s)$</li><li>$rs$是一个RE，L(rs) = L(r)L(s)$</li><li>$r^<em>$是一个RE，L(r^</em>)=L(r)^*$</li><li>$(r)$是一个RE，L((r))=L(r)$ </li></ul></li></ul><p>运算优先级: $* &gt; $ 连接 $&gt; |$</p><h3 id="正则定义"><a href="#正则定义" class="headerlink" title="正则定义"></a>正则定义</h3><p>正则定义就相当于一个函数封装。</p><h2 id="有穷自动机（FA）"><a href="#有穷自动机（FA）" class="headerlink" title="有穷自动机（FA）"></a>有穷自动机（FA）</h2><h3 id="转换图"><a href="#转换图" class="headerlink" title="转换图"></a>转换图</h3><p>初始状态用$\Rightarrow$表示, 终止状态用双圈表示。  </p><p>由一个有穷自动机$M$接收的所有串构成的集合称为是FA定义（或接收）的语言，记为$L(M)$。  </p><p><img alt="有穷自动机" data-src="/posts/b4e4b501/fa.png" class="lazyload"></p><h3 id="最长字串匹配原则"><a href="#最长字串匹配原则" class="headerlink" title="最长字串匹配原则"></a>最长字串匹配原则</h3><p>当输入串的多个前缀与一个或者多个模式匹配时，总是选择最长的前缀进行匹配。  </p><p>说人话就是到了终止状态，若还有符号，则继续前进。</p><h3 id="确定的有穷自动机（DFA）"><a href="#确定的有穷自动机（DFA）" class="headerlink" title="确定的有穷自动机（DFA）"></a>确定的有穷自动机（DFA）</h3><script type="math/tex; mode=display">M = (S, \Sigma,\delta,s_0, F)</script><p>$S$：有穷状态集</p><p>$\Sigma$：输入字母表</p><p>$\delta$：将$S\times\Sigma$映射到$S$的转换函数，$\delta(s,a)$表示从s出发，沿a能到达的状态</p><p>$s_0$：开始状态（或初始状态），$s_0\in S$</p><p>$F$：接收状态（或终止状态）集合，$F\subseteq S$</p><p>DFA也可以用转换表来表示</p><p><img alt="dfa" data-src="/posts/b4e4b501/dfa.png" class="lazyload">  </p><h3 id="不确定的有穷自动机"><a href="#不确定的有穷自动机" class="headerlink" title="不确定的有穷自动机"></a>不确定的有穷自动机</h3><script type="math/tex; mode=display">M = (S, \Sigma,\delta,s_0, F)</script><p>和DFA的区别在于，$\delta(s,a)$表示从s出发，沿a能到达的<strong>所有状态的集合</strong>。  </p><p><img alt="nfa" data-src="/posts/b4e4b501/nfa.png" class="lazyload"></p><h3 id="NFA和DFA的等价性"><a href="#NFA和DFA的等价性" class="headerlink" title="NFA和DFA的等价性"></a>NFA和DFA的等价性</h3><p><img alt="NFA和DFA的等价性" data-src="/posts/b4e4b501/nfaeqdfa.png" class="lazyload"></p><p>在上图的DFA中，状态1表示串以a结尾，状态2表示串以ab结尾，状态3表示串以abb结尾。  </p><p>他们都是$r=(a|b)^*abb$，由此$RE \Leftrightarrow FA \Leftrightarrow 正则文法$。</p><h3 id="带有和不带有-epsilon-边的NFA的等价性"><a href="#带有和不带有-epsilon-边的NFA的等价性" class="headerlink" title="带有和不带有$\epsilon$边的NFA的等价性"></a>带有和不带有$\epsilon$边的NFA的等价性</h3><p><img alt data-src="/posts/b4e4b501/knfa.png" class="lazyload"></p><h3 id="从正则表达式到有穷自动机"><a href="#从正则表达式到有穷自动机" class="headerlink" title="从正则表达式到有穷自动机"></a>从正则表达式到有穷自动机</h3><h4 id="根据RE构造NFA"><a href="#根据RE构造NFA" class="headerlink" title="根据RE构造NFA"></a>根据RE构造NFA</h4><p><img alt data-src="/posts/b4e4b501/re2nfa1.png" class="lazyload"></p><p><img alt data-src="/posts/b4e4b501/re2nfa2.png" class="lazyload"></p><p><img alt data-src="/posts/b4e4b501/re2nfa3.png" class="lazyload"></p><h4 id="从NFA到DFA的转换"><a href="#从NFA到DFA的转换" class="headerlink" title="从NFA到DFA的转换"></a>从NFA到DFA的转换</h4><p>子集构造法。</p><p><img alt data-src="/posts/b4e4b501/nfa2dfa1.png" class="lazyload"></p><p><img alt data-src="/posts/b4e4b501/nfa2dfa2.png" class="lazyload"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;参考课程为自哈尔滨工业大学的 &lt;a href=&quot;https://www.icourse163.org/learn/HIT-1002123007#/learn/announce&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;《编译原理》&lt;/a&gt;，参考教材为《
      
    
    </summary>
    
    
      <category term="编译原理" scheme="http://blog.logan-qiu.cn/categories/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/"/>
    
    
      <category term="编译原理" scheme="http://blog.logan-qiu.cn/tags/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/"/>
    
      <category term="期末复习" scheme="http://blog.logan-qiu.cn/tags/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>DenseNet论文阅读笔记</title>
    <link href="http://blog.logan-qiu.cn/posts/ef2be802/"/>
    <id>http://blog.logan-qiu.cn/posts/ef2be802/</id>
    <published>2019-12-08T05:01:31.000Z</published>
    <updated>2019-12-11T10:22:48.836Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/pdf/1608.06993.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1608.06993.pdf</a></p><p>官方实现和预训练模型：<a href="https://github.com/liuzhuang13/DenseNet" target="_blank" rel="noopener">https://github.com/liuzhuang13/DenseNet </a></p><h2 id="相关背景"><a href="#相关背景" class="headerlink" title="相关背景"></a>相关背景</h2><p>本篇在16年8月挂到arXiv上，中了2017年CVPR，是继16年何大神的ResNet之后，第二个华人的best paper，<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F53417625" target="_blank" rel="noopener">这里</a>有个作者本尊的talk，现场讲解。一作Gao Huang（黄高）05年北航的本科生（GPA第一），15年清华博士毕业（读了6年。。），后来在康奈尔待了3年做博后，此刻在清华作青椒，本篇是在康奈尔时的工作。二作刘壮（同等贡献）也是碉堡，现在在伯克利做博士生，之前是清华姚班的（13级），发这篇文章时还在清华，也就是说<a href="https://links.jianshu.com/go?to=http%3A%2F%2Fiiis.tsinghua.edu.cn%2Fshow-6425-1.html" target="_blank" rel="noopener">本科生</a>。。。最近以一作的身份新发了一篇《Rethinking the Value of Network Pruning》，中了19年的ICLR，同时也是18年NIPS的best paper award。。这个世界太疯狂了，这都不是潜力股了，而是才华横溢溢的不行了。$^{[1]}$</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>作者说在之前的一些研究中，发现了在卷积神经网络中增加一些捷径（<em>shorter connections</em>，看过ResNet的应该明白）。DenseNet提出来的新的方法是，把所有层提取出来的feature map都传递到后面的层中。同时，作者提出了DenseNet的一些优势：缓解梯度消失、加强特征传递、特征复用和显著减少训练参数。他们将DenseNet应用于一些物体识别 的任务，发现在使用更少参数的情况下还能获得比其他网络更好地效果。</p><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>这篇文章的引言部分呢，把摘要复读了一遍。不过在第三段讲DenseNet和ResNet区别的时候，提出了一个concatenating的概念。我第一次读的时候在这个词的理解上花费了很大的功夫。其实这个concatenating 就是把上层的所有feature map直接连接在下层的feature map后面，这样的话每一层都会包含之前的所有信息。</p><h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h2><p>作者说，对网络结构的探索是神经网络研究的重要工作。Highway Networks和ResNet让网络go deeper，而Inception Network能让网络go wider。而DenseNet使用的是完全不同的方法，作者强调特征复用，制造更简单有效的网络。$^{[2]}$</p><h2 id="3-DenseNet"><a href="#3-DenseNet" class="headerlink" title="3. DenseNet"></a>3. DenseNet</h2><p>从这里开始，正式介绍DenseNet，是这篇文章的干货。</p><p>作者假设有一张图片$x_0$被输入到一个L层的卷积神经网络中，这个网络的每一层都使用了一种非线性变换$H_l(·)$中。这个$H_l(·)$是实现了Batch Normalization，ReLU，池化和卷积之类的操作的复合函数。假设每一层的输出为$x_l$。</p><h3 id="3-1-ResNets"><a href="#3-1-ResNets" class="headerlink" title="3.1 ResNets"></a>3.1 ResNets</h3><p>其实这一部分是和4.2对比着写的，也是为了突出DenseNet的不同之处。传统的卷积网络把第$l$层的输出作为$l+1$层的输入。ResNet增加了一个跳跃连接，就相当于$x_l = H_l(x_{l-1}) + x_{l-1}$ 。作者这么说，其实就是相当于用自定义的$H_l(·)$函数来解释一些下ResNet里面的那个$a_{l+2} = g(z_{l+2} + a_l)$。也就是说，在ResNet中，复合函数$H_l(·)$就是一个Conv+Pool+ReLu+Conv+Pool。</p><h3 id="3-2-Dense-connectivityxw"><a href="#3-2-Dense-connectivityxw" class="headerlink" title="3.2 Dense connectivityxw"></a>3.2 Dense connectivityxw</h3><p>这一部分是作者提出DenseNet的连接方式，我们看作者提出的式子:</p><script type="math/tex; mode=display">x_l = H_l([x_0, x_1, x_2, \dots, x_{l-1}]) \tag{1}</script><p>先不管$H_l(·)$的具体实现，这个式子可以说明每一层的特征图都会包含之前所有特征。同时，观察图3.2.1和图3.2.2，由于DenseNet强调特征复用，所以他的每一个卷积层能够变得更“slim”， 这也是DenseNet所用的参数比较少的一个原因。对于图3.2.3，我们可以发现，对于$L$层的网络，DenseNet总共需要$\frac{L(L+1)}{2}$次连接。</p><p><img alt="图3.2.1 直接连接方式" data-src="/posts/ef2be802/standardconnect.png" class="lazyload"></p><p><img alt="图3.2.2 Dense连接1" data-src="/posts/ef2be802/denseconnect.png" class="lazyload"></p><p><img alt="图3.2.3 Dense连接2" data-src="/posts/ef2be802/cover.png" class="lazyload"></p><h3 id="3-3-Composite-function"><a href="#3-3-Composite-function" class="headerlink" title="3.3 Composite function"></a>3.3 Composite function</h3><p>这个部分作者对复合函数进行了非常简短的说明，$H_l(x) = BN - ReLU - Conv$，也就是先做Batch Normalization，再做ReLU最后做卷积。这里作者没有说明为什么先用激活函数再卷积，但是可能和<a href="https://arxiv.org/abs/1603.05027" target="_blank" rel="noopener">Identity mappings in deep residual networks</a>这篇文章有关系。</p><h3 id="3-4-Pooling-layers"><a href="#3-4-Pooling-layers" class="headerlink" title="3.4 Pooling layers"></a>3.4 Pooling layers</h3><p>在式1中，要把$x_0, x_1, x_2, \dots, x_{l-1}$这些特征连接起来，需要他们有相同的维度。所以说不能直接在Conv后面接池化。为了在网络中使用池化，作者把网络划分成多个dense block。如图3.2.3即为一个denseblock。而在dense block之间，作者提出了transition层，用于做卷积和池化操作。transition层包括了一个Batch Normaliztion，一个1x1卷积和一个2x2 average pooling。</p><p><img alt="图3.4.1 Dense block" data-src="/posts/ef2be802/denseblock.png" class="lazyload"></p><h3 id="3-5-Growth-rate"><a href="#3-5-Growth-rate" class="headerlink" title="3.5 Growth rate"></a>3.5 Growth rate</h3><p>如果说，每个$H_l$输出了k张feature map，那么在第$l$层，总共会有$k_0 + k × (l − 1)$个feature map。作者把这里的$k$定义为增长率，在后文的实现部分，作者在实验部分说明了即使用很小的增长率也可以获得STOA的效果。</p><h3 id="3-6-Bottleneck-layers"><a href="#3-6-Bottleneck-layers" class="headerlink" title="3.6 Bottleneck layers"></a>3.6 Bottleneck layers</h3><p>瓶颈层是在3x3卷积之前加入1x1的卷积，作者发现使用瓶颈层在DenseNet上尤其有效。加入瓶颈层以后复合函数就变成了$H_l(x) = BN-ReLU-Conv(1× 1)-BN-ReLU-Conv(3×3)  $。运用了瓶颈层的DenseNet就叫DenseNet-B。</p><h3 id="3-7-Compression"><a href="#3-7-Compression" class="headerlink" title="3.7 Compression"></a>3.7 Compression</h3><p>这里所谓的压缩，就是在transition层中，减少feature map的数量，从而达到进一步“ensilm”模型的效果。作者定义了压缩因子$\theta$，如果说原本有m个feature map，那么只保留$\lfloor \theta*x \rfloor$个feature map。运用了Compression的DenseNet就叫DenseNet-C，如果即使用了瓶颈层，又用了Compression，就是DenseNet-BC了。  </p><h2 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h2><p>文章后面是一些实验，不再详细说明了。在这篇博客里面，有一些不完美的地方，比如3.3中，先ReLU再Conv的原因、3.4中AP和MP的选择、3.7中compression的实现方法等等。可能作进一步了解以后可以更新本篇。</p><hr><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] 论文阅读 | CVPR2017(Best Paper) | Densely Connected Convolutional Networks, <a href="https://www.jianshu.com/p/cced2e8378bf" target="_blank" rel="noopener">https://www.jianshu.com/p/cced2e8378bf</a>, 2019.3</p><p>[2] DenseNet 论文阅读笔记, <a href="https://www.cnblogs.com/zhhfan/p/10187634.html" target="_blank" rel="noopener">https://www.cnblogs.com/zhhfan/p/10187634.html</a>, 2018.12</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/pdf/1608.06993.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;官方实现和预
      
    
    </summary>
    
    
      <category term="深度学习" scheme="http://blog.logan-qiu.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://blog.logan-qiu.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="理论" scheme="http://blog.logan-qiu.cn/tags/%E7%90%86%E8%AE%BA/"/>
    
      <category term="DenseNet" scheme="http://blog.logan-qiu.cn/tags/DenseNet/"/>
    
      <category term="论文阅读" scheme="http://blog.logan-qiu.cn/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>关于这首柳永的《鹤冲天》</title>
    <link href="http://blog.logan-qiu.cn/posts/da209311/"/>
    <id>http://blog.logan-qiu.cn/posts/da209311/</id>
    <published>2019-12-06T11:40:42.000Z</published>
    <updated>2019-12-07T17:02:14.028Z</updated>
    
    <content type="html"><![CDATA[<div>  <div style="width:50%;margin:0 auto;">    <center>鹤冲天·黄金榜上</center>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;黄金榜上，偶失龙头望。明代暂遗贤，如何向。未遂风云便，争不恣狂荡。何须论得丧？才子词人，自是白衣卿相。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;烟花巷陌，依约丹青屏障。幸有意中人，堪寻访。且恁偎红倚翠，风流事，平生畅。青春都一饷。忍把浮名，换了浅斟低唱。  </div></div>  <hr><p>有人说，柳永的词格调不高。确实不高，单看上阕的牢骚话倒是觉得不错，尤其是“才子词人，自是白衣卿相”，把柳永对自身才气的自信和对落榜$^{[1]}$的不甘展现的淋漓尽致。然读到下阙的时候，格调开始逐渐走低。当读到“青春都一饷”的时候，我期待着作者漂亮的结句来点睛，却等来了一个“忍把浮名”。这就很操蛋了，若是只有“把浮名换了浅斟低唱”已是下乘，加上“忍”字便是落入下下之流。于是整篇词的情感变化变成了：自闭了—&gt;尝试想开—&gt;我又自闭了的过程。有人给这首词打上了豪迈的标签，但是在我看来，所有的豪迈都毁在了“忍”字上。   </p><p>这是柳永的早期作品，大概更能够和年轻人产生共鸣吧，我认为这首词的动人之处，在于真实。你看作者的语言：“如何向”、“争不”、“且恁”，失意了，发发牢骚。但是牢骚发完，该伤感还得伤感，绝不为了格调来一个“一蓑烟雨任平生”。烟花巷陌，是柳永寻欢的场所，他要去找他的意中人，诉说心中的烦闷。但是这还不够，最后还要酸一把、嘴硬一下，<strong>忍把浮名，换了浅斟低唱</strong>。这个毁了全词文学价值的字，是柳永真情的流露，让我们能够感受柳永的内心，体会柳永的性情。也是这个“忍”字出来的时候，我发现了我和柳永性格上的相似，这一刻，我们仿佛成为了相识多年的旧友，这种感觉太美妙了。（颇有小杠精找朋友的感觉）</p><hr><p>[1] 现在好像大部分理解为未中状元，我认为落榜更加合适一些。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;div&gt;
  &lt;div style=&quot;width:50%;margin:0 auto;&quot;&gt;
    &lt;center&gt;鹤冲天·黄金榜上&lt;/center&gt;
    &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;黄金榜上，偶失龙头望。
      
    
    </summary>
    
    
      <category term="杂谈" scheme="http://blog.logan-qiu.cn/categories/%E6%9D%82%E8%B0%88/"/>
    
    
      <category term="诗词" scheme="http://blog.logan-qiu.cn/tags/%E8%AF%97%E8%AF%8D/"/>
    
  </entry>
  
  <entry>
    <title>深度学习之卷积神经网络</title>
    <link href="http://blog.logan-qiu.cn/posts/17556c44/"/>
    <id>http://blog.logan-qiu.cn/posts/17556c44/</id>
    <published>2019-12-05T09:01:49.000Z</published>
    <updated>2019-12-06T03:11:54.829Z</updated>
    
    <content type="html"><![CDATA[<p>卷积神经网络在计算机视觉领域取得了巨大的成功，甚至在音频处理等其他领域，卷积神经网络也能发挥较好的效果。那么，卷积到底是怎么样的一种运算呢？为什么卷积神经网络在图像处理上会有这么大的优势呢？让我们一起深入研究卷积神经网络的原理，学习卷积神经网络，能让你处理远比MNIST数据集复杂的图片，开启视觉计算的新领域。</p><h2 id="什么是卷积？"><a href="#什么是卷积？" class="headerlink" title="什么是卷积？"></a>什么是卷积？</h2><h3 id="二维卷积"><a href="#二维卷积" class="headerlink" title="二维卷积"></a>二维卷积</h3><p>卷积（convolution），其实就是一种数学运算。让我们来看一下面的这个卷积运算的例子：</p><script type="math/tex; mode=display">\left[ \begin{matrix}   1 & 2 & 3 & 1 & 2 & 3  \\   4 & 5 & 6 & 4 & 5 & 6  \\   7 & 8 & 9 &7 & 8 & 9 \\ 1 & 2 & 3 & 1 & 2 & 3 \\   4 & 5 & 6 & 4 & 5 & 6  \\   7 & 8 & 9 &7 & 8 & 9 \end{matrix}  \right] * \left[ \begin{matrix}  1 & 0 & -1 \\ 1 & 0 & -1 \\1 & 0 & -1 \end{matrix}\right] = \left[ \begin{matrix} -6 &3&3&-6\\-6 &3&3&-6\\-6 &3&3&-6\\-6 &3&3&-6\end{matrix}\right] \tag{1}</script><p>事实上，卷积是一种非常简单的运算，与矩阵乘法不同的是，他对矩阵维度要求不是很高，允许不同大小的矩阵进行运算。为了方便说明，我们首先定义一下几个说法。</p><ul><li><p>(1)式中卷积符号左侧的6x6矩阵叫<strong>被卷积矩阵</strong></p></li><li><p>(1)式中卷积符号右侧的3x3矩阵叫做<strong>卷积核</strong></p></li></ul><p>卷积运算的步骤如下：  </p><ol><li><p>把卷积核“覆盖”到被卷积矩阵左上角的位置，将对应元素相乘并相加，作为结果矩阵中的左上角元素。（1）式中的运算过程如下：</p><script type="math/tex; mode=display">1\times1+0\times2+(-1)\times3+1\times4+0\times5+(-1)\times6+1\times7+0\times8+(-1)\times9=-6</script></li><li><p>把卷积核右移一格，重复进行1操作。如果到行尾，则换行继续，直到“覆盖”到右下角元素为止（这一步有点像滑动窗口）。</p></li></ol><p>通过GIF可以形象地说明这一运算的过程：</p><p><img alt data-src="/posts/17556c44/conv.gif" class="lazyload"></p><p><em>不过，我们这里的卷积和数学上讲的卷积有一些区别。数学上的卷积要多一个把被卷积矩阵旋转180度的操作。</em></p><h3 id="三维卷积"><a href="#三维卷积" class="headerlink" title="三维卷积"></a>三维卷积</h3><p>由于图像有可能是彩色的，因此图像组成的矩阵会变成 <code>宽度</code>x<code>高度</code>x<code>通道数量</code>的三阶矩阵。彩色图片的通道数（num of channels）一般为3。然而，三维而计算方法和二维的计算方法是一样的，只是原本的3x3的卷积核变为3x3x3的而已，求和的时候变成27个数相加，最后结果是一个二阶矩阵。</p><h2 id="为什么卷积能用于视觉领域"><a href="#为什么卷积能用于视觉领域" class="headerlink" title="为什么卷积能用于视觉领域"></a>为什么卷积能用于视觉领域</h2><p>利用卷积运算，计算机可以完成<strong>边缘检测</strong>的任务。假如有矩阵如下：</p><script type="math/tex; mode=display">\left[ \begin{matrix} 0&0&1&1&0&0\\0&0&1&1&0&0\\0&0&1&1&0&0\\0&0&1&1&0&0\\0&0&1&1&0&0\\0&0&1&1&0&0 \end{matrix} \right]</script><p>如果用(1)式中的卷积核，对这张图片做卷积，结果会是这样的：</p><script type="math/tex; mode=display">\left[ \begin{matrix} 10&10&10&0&0&0\\10&10&10&0&0&0\\10&10&10&0&0&0\\10&10&10&0&0&0\\10&10&10&0&0&0\\10&10&10&0&0&0 \end{matrix} \right] * \left[ \begin{matrix}  1 & 0 & -1 \\ 1 & 0 & -1 \\1 & 0 & -1 \end{matrix}\right] = \left[ \begin{matrix} 0&30&30&0\\0&30&30&0\\0&30&30&0\\0&30&30&0\end{matrix}\right]\tag{2}</script><p>如果把他们转化成图片，可以很直观的看到，原图左侧是黑色，右侧为白色；而图像经过卷积以后变成中间一条黑色，两边是白色。这说明了原本黑白交接的边界线被卷积识别出来了。因此，(1)(2)中的卷积核实际上是起到了检测图片垂直边缘的作用。同样地，如果给这个卷积核加上一个90度的旋转变换，可以得到一个水平边缘检测器。不难想象，如果在神经网络中使用卷积运算，网络就可以通过拟合卷积核中的参数来检测不同方向上的边缘。这就是卷积提取图像特征的方式。</p><p>事实上，卷积神经网络在一个卷积层中会使用多个卷积核。每一个卷积核进行卷积运算都会得到一个二阶矩阵，这个矩阵我们把他叫做<strong>feature map</strong>(特征图)。使用n个卷积核就会形成n个feature map。最后我们会把这些feature map“叠”在一起，进入下一层计算，下一层的输入通道数即为n。</p><p><img alt data-src="/posts/17556c44/edge_detect.jpg" class="lazyload"></p><h2 id="卷积运算的常用参数"><a href="#卷积运算的常用参数" class="headerlink" title="卷积运算的常用参数"></a>卷积运算的常用参数</h2><h3 id="卷积核大小"><a href="#卷积核大小" class="headerlink" title="卷积核大小"></a>卷积核大小</h3><p>卷积核大小，顾名思义，就是卷积核矩阵的维度。通常为奇数，常见的取值有3x3，5x5和7x7。现在一般认为小卷积核的效果比较好。一个7x7的卷积，其实可以用两个3x3卷积来代替。</p><h3 id="Padding和Stride"><a href="#Padding和Stride" class="headerlink" title="Padding和Stride"></a>Padding和Stride</h3><p>padding（填充的层数）: 我们从(1)的例子中可以看到，做完卷积运算以后，图片变小了（从6x6变成了4x4）。这不利于进一步提取图像特征。因此，我们可以使用padding来填充图像外围区域，比如填0，来把原图变大，这样可以保证卷积后图片的大小。</p><p>stride（步长）：指的是在“滑动窗口”的过程中，每次滑动的步长。你可以对横向和纵向分别设置。</p><h2 id="卷积的好伙伴——池化（Pooling）"><a href="#卷积的好伙伴——池化（Pooling）" class="headerlink" title="卷积的好伙伴——池化（Pooling）"></a>卷积的好伙伴——池化（Pooling）</h2><p>观察(2)中的卷积结果，可以发现用卷积做边缘检测时，边界线被“弄粗”了。这个时候我们可以用池化来解决这个问题。池化也是一种非常简单的数学运算，我们还是先来看一个例子：</p><script type="math/tex; mode=display">\left[ \begin{matrix} 0&0&30&30&0&0\\0&0&30&30&0&0\\0&0&30&30&0&0\\0&0&30&30&0&0\\0&0&30&30&0&0\\0&0&30&30&0&0 \end{matrix} \right]\overset{2\times2MP,s=2}\longrightarrow \left[ \begin{matrix} 0&30&0\\0&30&0\\0&30&0 \end{matrix} \right] \tag{3}</script><p>还是那卷积运算中的滑动窗口的眼光来看，这里用了2x2池化，也就是用2x2的窗口去套原矩阵，并且选择2x2窗口中的最大值，组成新的矩阵。你也可以选择2x2窗口中的平均值，这是两种不同的池化方式，一个叫Max pooling（最大池化），另一个叫Average pooling（平均池化）。我们一般用缩写MP和AP来表示。  </p><p>池化还有一个好处，他可以把feature map缩小，减少运算数量。同时，池化是不需要参数的，也就是不参与训练，不会耗费训练时间。</p><h2 id="卷积神经网络（CNN）"><a href="#卷积神经网络（CNN）" class="headerlink" title="卷积神经网络（CNN）"></a>卷积神经网络（CNN）</h2><p>卷积神经网络由输入层，卷积层和全连接层构成。</p><h3 id="卷积层（Conv）"><a href="#卷积层（Conv）" class="headerlink" title="卷积层（Conv）"></a>卷积层（Conv）</h3><p>卷积层就是把图像对多个卷积核进行卷积，然后再对得到的feature map做池化处理。<strong>一个卷积层通常由卷积+池化组成</strong>（大概是池化没有参数，所有不配单独成池化层233333）。  </p><p>在卷积层中，出了刚刚提到的卷积参数以外，还有一个超参数需要配置，就是卷积核的个数。这个在上文也已有介绍。</p><h3 id="全连接层（FC）"><a href="#全连接层（FC）" class="headerlink" title="全连接层（FC）"></a>全连接层（FC）</h3><p>一个CNN网络通常会包含多个卷积层，然后在后面接一个或者多个全连接层。全连接层实际上就是以前接触过的普通的神经网络中的层。负责把conv层提取到的特征进行归纳计算。</p><h3 id="卷积层的优势"><a href="#卷积层的优势" class="headerlink" title="卷积层的优势"></a>卷积层的优势</h3><ol><li>卷积运算量不大，可以高效提取特征信息。</li><li>池化层可以大大减少运算量，加快训练速度。</li><li>多个卷积层可以把图片中的参数变少，比如128x128x3的图片，经过数轮卷积层以后变成7x7x128的feature map，图片参数从49152减小到6272。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;卷积神经网络在计算机视觉领域取得了巨大的成功，甚至在音频处理等其他领域，卷积神经网络也能发挥较好的效果。那么，卷积到底是怎么样的一种运算呢？为什么卷积神经网络在图像处理上会有这么大的优势呢？让我们一起深入研究卷积神经网络的原理，学习卷积神经网络，能让你处理远比MNIST数据
      
    
    </summary>
    
    
      <category term="深度学习" scheme="http://blog.logan-qiu.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://blog.logan-qiu.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="理论" scheme="http://blog.logan-qiu.cn/tags/%E7%90%86%E8%AE%BA/"/>
    
      <category term="CNN" scheme="http://blog.logan-qiu.cn/tags/CNN/"/>
    
      <category term="卷积神经网络" scheme="http://blog.logan-qiu.cn/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Hello hexo+Butterfly</title>
    <link href="http://blog.logan-qiu.cn/posts/8ab1f019/"/>
    <id>http://blog.logan-qiu.cn/posts/8ab1f019/</id>
    <published>2019-12-03T13:23:35.000Z</published>
    <updated>2019-12-05T13:54:37.331Z</updated>
    
    <content type="html"><![CDATA[<p>我原是想自己写一个完整的博客前后端，现在看身边的小伙伴都开始交换友链了，再想想即将到来的期末考试，还有自己拖下的一些项目，还是算了吧。只能这样安慰自己：明明已经有这么多现成的开源项目了，何必重复劳动，手撸轮子。既然是自己的博客，可以多发表发表自己的观点，大概不会像用博客园那样有拘束感。</p><p>第一篇就讲一讲搭建Hexo时遇到的一些问题吧。</p><h2 id="Hexo的安装"><a href="#Hexo的安装" class="headerlink" title="Hexo的安装"></a>Hexo的安装</h2><p>如果是使用Hexo+GitxxPage的话，无需把Hexo安装在服务器上，只要安装在本机上即可。因为到时候使用的是gitxx的服务器，本机直接<code>deploy</code>比较方便。</p><p>首先是安装一些依赖环境，git啦，node啦，这些都不细说。主要是讲用npm装hexo本体的时候的这行命令。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">npm install hexo-cli -g</span></pre></td></tr></table></figure><p>其中的install在有的教程里面用的是缩写i，这是等价的。后面的 <code>-g</code> 参数意味着全局安装。也就是说在任意目录下，你都可以运行hexo。</p><h3 id="MacOS系统中的问题"><a href="#MacOS系统中的问题" class="headerlink" title="MacOS系统中的问题"></a>MacOS系统中的问题</h3><p>mac通过npm安装的时候可能会报一堆权限错误的问题，即使加了<code>sudo</code>，也有可能造成安装卡住不动的问题。这是由于访问<code>/usr/local</code>目录权限不足造成的，这时候就要使用chown来解决。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">sudo chown R $USER /usr/local</span></pre></td></tr></table></figure><h3 id="Linux系统中的问题"><a href="#Linux系统中的问题" class="headerlink" title="Linux系统中的问题"></a>Linux系统中的问题</h3><p>我装nodejs的时候是直接下载二进制包解压，然后配置软链接的。所以全局安装的包有可能不在环境变量下，还需要再把<code>node/bin</code>放入环境变量中，才能生效。Windows版本可能也有环境变量的问题，需要注意。</p><h2 id="配置SSH密钥连接GitHub账户"><a href="#配置SSH密钥连接GitHub账户" class="headerlink" title="配置SSH密钥连接GitHub账户"></a>配置SSH密钥连接GitHub账户</h2><p>一开始配ssh密钥的时候github一直提示，密钥格式错误，其实是复制错了文件。生成文件的时候他可能会告诉你生成的是<code>~/.ssh/id_rsa</code>，其实要复制的是<code>~/.ssh/id_rsa.pub</code>文件。同时还需要注意的是要使用cat命令而不是vim命令打开文件，用vim复制可能会包含多余的字符。</p><h2 id="Butterfly主题的使用"><a href="#Butterfly主题的使用" class="headerlink" title="Butterfly主题的使用"></a>Butterfly主题的使用</h2><p>跟着官方文档一步一步走就行了。复制配置文件butterfly.yml那一步可能会让人感到困惑。被复制的文件应该是<code>themes/Butterfly/_config.yml</code>，而不是博客文件根目录下的那个配置文件。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;我原是想自己写一个完整的博客前后端，现在看身边的小伙伴都开始交换友链了，再想想即将到来的期末考试，还有自己拖下的一些项目，还是算了吧。只能这样安慰自己：明明已经有这么多现成的开源项目了，何必重复劳动，手撸轮子。既然是自己的博客，可以多发表发表自己的观点，大概不会像用博客园那
      
    
    </summary>
    
    
      <category term="杂谈" scheme="http://blog.logan-qiu.cn/categories/%E6%9D%82%E8%B0%88/"/>
    
    
      <category term="Hexo" scheme="http://blog.logan-qiu.cn/tags/Hexo/"/>
    
      <category term="Butterfly" scheme="http://blog.logan-qiu.cn/tags/Butterfly/"/>
    
  </entry>
  
</feed>
