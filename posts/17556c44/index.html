<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>深度学习之卷积神经网络 | QF的个人博客</title><meta name="description" content="深度学习之卷积神经网络"><meta name="keywords" content="深度学习,理论,CNN,卷积神经网络"><meta name="author" content="QF"><meta name="copyright" content="QF"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="preconnect" href="//cdn.jsdelivr.net"><meta name="google-site-verification" content="sNaFjYbg6n7muvNP29fqjHjRmtBZHvPpYCaYe9br2DI"><meta name="baidu-site-verification" content="bBvl9tNq5t"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="深度学习之卷积神经网络"><meta name="twitter:description" content="深度学习之卷积神经网络"><meta name="twitter:image" content="http://blog.logan-qiu.cn/posts/17556c44/cover.png"><meta property="og:type" content="article"><meta property="og:title" content="深度学习之卷积神经网络"><meta property="og:url" content="http://blog.logan-qiu.cn/posts/17556c44/"><meta property="og:site_name" content="QF的个人博客"><meta property="og:description" content="深度学习之卷积神经网络"><meta property="og:image" content="http://blog.logan-qiu.cn/posts/17556c44/cover.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="http://blog.logan-qiu.cn/posts/17556c44/"><link rel="prev" title="关于这首柳永的《鹤冲天》" href="/http:/blog.logan-qiu.cn/posts/da209311/"><link rel="next" title="Hello hexo+Butterfly" href="/http:/blog.logan-qiu.cn/posts/8ab1f019/"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"http://blog.logan-qiu.cn/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'true',
  highlight_lang: 'true',
  highlight_shrink: 'false',
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  copyright: {"languages":{"author":"作者: QF","link":"链接: http://blog.logan-qiu.cn/posts/17556c44/","source":"来源: QF的个人博客","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  copy_copyright_js: true,
  ClickShowText: undefined,
  medium_zoom: 'false',
  Snackbar: undefined
  
}</script><link rel="alternate" href="/atom.xml" title="QF的个人博客" type="application/atom+xml">
</head><body><div id="header"> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">QF的个人博客</a></span><i class="fa fa-bars fa-fw toggle-menu pull_right close" aria-hidden="true"></i><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div></div></span><span class="pull_right" id="search_button"></span></div></div><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="lazyload avatar_img" src="/static/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'"></div><div class="mobile_post_data"><div class="mobile_data_item is_center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">9</div></a></div></div><div class="mobile_data_item is_center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">14</div></a></div></div><div class="mobile_data_item is_center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">5</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#什么是卷积？"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">什么是卷积？</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#二维卷积"><span class="toc_mobile_items-number">1.1.</span> <span class="toc_mobile_items-text">二维卷积</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#三维卷积"><span class="toc_mobile_items-number">1.2.</span> <span class="toc_mobile_items-text">三维卷积</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#为什么卷积能用于视觉领域"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">为什么卷积能用于视觉领域</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#卷积运算的常用参数"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">卷积运算的常用参数</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#卷积核大小"><span class="toc_mobile_items-number">3.1.</span> <span class="toc_mobile_items-text">卷积核大小</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Padding和Stride"><span class="toc_mobile_items-number">3.2.</span> <span class="toc_mobile_items-text">Padding和Stride</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#卷积的好伙伴——池化（Pooling）"><span class="toc_mobile_items-number">4.</span> <span class="toc_mobile_items-text">卷积的好伙伴——池化（Pooling）</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#卷积神经网络（CNN）"><span class="toc_mobile_items-number">5.</span> <span class="toc_mobile_items-text">卷积神经网络（CNN）</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#卷积层（Conv）"><span class="toc_mobile_items-number">5.1.</span> <span class="toc_mobile_items-text">卷积层（Conv）</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#全连接层（FC）"><span class="toc_mobile_items-number">5.2.</span> <span class="toc_mobile_items-text">全连接层（FC）</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#卷积层的优势"><span class="toc_mobile_items-number">5.3.</span> <span class="toc_mobile_items-text">卷积层的优势</span></a></li></ol></li></ol></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#什么是卷积？"><span class="toc-number">1.</span> <span class="toc-text">什么是卷积？</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#二维卷积"><span class="toc-number">1.1.</span> <span class="toc-text">二维卷积</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#三维卷积"><span class="toc-number">1.2.</span> <span class="toc-text">三维卷积</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#为什么卷积能用于视觉领域"><span class="toc-number">2.</span> <span class="toc-text">为什么卷积能用于视觉领域</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#卷积运算的常用参数"><span class="toc-number">3.</span> <span class="toc-text">卷积运算的常用参数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#卷积核大小"><span class="toc-number">3.1.</span> <span class="toc-text">卷积核大小</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Padding和Stride"><span class="toc-number">3.2.</span> <span class="toc-text">Padding和Stride</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#卷积的好伙伴——池化（Pooling）"><span class="toc-number">4.</span> <span class="toc-text">卷积的好伙伴——池化（Pooling）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#卷积神经网络（CNN）"><span class="toc-number">5.</span> <span class="toc-text">卷积神经网络（CNN）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#卷积层（Conv）"><span class="toc-number">5.1.</span> <span class="toc-text">卷积层（Conv）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#全连接层（FC）"><span class="toc-number">5.2.</span> <span class="toc-text">全连接层（FC）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#卷积层的优势"><span class="toc-number">5.3.</span> <span class="toc-text">卷积层的优势</span></a></li></ol></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/posts/17556c44/cover.png)"><div id="post-info"><div id="post-title"><div class="posttitle">深度学习之卷积神经网络</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2019-12-05<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> 更新于 2019-12-06</time><span class="post-meta__separator mobile_hidden">|</span><span class="mobile_hidden"><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span><div class="post-meta-wordcount"><i class="fa fa-eye post-meta__icon" aria-hidden="true">       </i><span>阅读量: </span><span id="busuanzi_value_page_pv"></span></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><p>卷积神经网络在计算机视觉领域取得了巨大的成功，甚至在音频处理等其他领域，卷积神经网络也能发挥较好的效果。那么，卷积到底是怎么样的一种运算呢？为什么卷积神经网络在图像处理上会有这么大的优势呢？让我们一起深入研究卷积神经网络的原理，学习卷积神经网络，能让你处理远比MNIST数据集复杂的图片，开启视觉计算的新领域。</p>
<h2 id="什么是卷积？"><a href="#什么是卷积？" class="headerlink" title="什么是卷积？"></a>什么是卷积？</h2><h3 id="二维卷积"><a href="#二维卷积" class="headerlink" title="二维卷积"></a>二维卷积</h3><p>卷积（convolution），其实就是一种数学运算。让我们来看一下面的这个卷积运算的例子：</p>
<script type="math/tex; mode=display">\left[ \begin{matrix}   1 & 2 & 3 & 1 & 2 & 3  \\   4 & 5 & 6 & 4 & 5 & 6  \\   7 & 8 & 9 &7 & 8 & 9 \\ 1 & 2 & 3 & 1 & 2 & 3 \\   4 & 5 & 6 & 4 & 5 & 6  \\   7 & 8 & 9 &7 & 8 & 9 \end{matrix}  \right] * \left[ \begin{matrix}  1 & 0 & -1 \\ 1 & 0 & -1 \\1 & 0 & -1 \end{matrix}\right] = \left[ \begin{matrix} -6 &3&3&-6\\-6 &3&3&-6\\-6 &3&3&-6\\-6 &3&3&-6\end{matrix}\right] \tag{1}</script><p>事实上，卷积是一种非常简单的运算，与矩阵乘法不同的是，他对矩阵维度要求不是很高，允许不同大小的矩阵进行运算。为了方便说明，我们首先定义一下几个说法。</p>
<ul>
<li><p>(1)式中卷积符号左侧的6x6矩阵叫<strong>被卷积矩阵</strong></p>
</li>
<li><p>(1)式中卷积符号右侧的3x3矩阵叫做<strong>卷积核</strong></p>
</li>
</ul>
<p>卷积运算的步骤如下：  </p>
<ol>
<li><p>把卷积核“覆盖”到被卷积矩阵左上角的位置，将对应元素相乘并相加，作为结果矩阵中的左上角元素。（1）式中的运算过程如下：</p>
<script type="math/tex; mode=display">1\times1+0\times2+(-1)\times3+1\times4+0\times5+(-1)\times6+1\times7+0\times8+(-1)\times9=-6</script></li>
<li><p>把卷积核右移一格，重复进行1操作。如果到行尾，则换行继续，直到“覆盖”到右下角元素为止（这一步有点像滑动窗口）。</p>
</li>
</ol>
<p>通过GIF可以形象地说明这一运算的过程：</p>
<p><img alt data-src="/posts/17556c44/conv.gif" class="lazyload"></p>
<p><em>不过，我们这里的卷积和数学上讲的卷积有一些区别。数学上的卷积要多一个把被卷积矩阵旋转180度的操作。</em></p>
<h3 id="三维卷积"><a href="#三维卷积" class="headerlink" title="三维卷积"></a>三维卷积</h3><p>由于图像有可能是彩色的，因此图像组成的矩阵会变成 <code>宽度</code>x<code>高度</code>x<code>通道数量</code>的三阶矩阵。彩色图片的通道数（num of channels）一般为3。然而，三维而计算方法和二维的计算方法是一样的，只是原本的3x3的卷积核变为3x3x3的而已，求和的时候变成27个数相加，最后结果是一个二阶矩阵。</p>
<h2 id="为什么卷积能用于视觉领域"><a href="#为什么卷积能用于视觉领域" class="headerlink" title="为什么卷积能用于视觉领域"></a>为什么卷积能用于视觉领域</h2><p>利用卷积运算，计算机可以完成<strong>边缘检测</strong>的任务。假如有矩阵如下：</p>
<script type="math/tex; mode=display">\left[ \begin{matrix} 0&0&1&1&0&0\\0&0&1&1&0&0\\0&0&1&1&0&0\\0&0&1&1&0&0\\0&0&1&1&0&0\\0&0&1&1&0&0 \end{matrix} \right]</script><p>如果用(1)式中的卷积核，对这张图片做卷积，结果会是这样的：</p>
<script type="math/tex; mode=display">\left[ \begin{matrix} 10&10&10&0&0&0\\10&10&10&0&0&0\\10&10&10&0&0&0\\10&10&10&0&0&0\\10&10&10&0&0&0\\10&10&10&0&0&0 \end{matrix} \right] * \left[ \begin{matrix}  1 & 0 & -1 \\ 1 & 0 & -1 \\1 & 0 & -1 \end{matrix}\right] = \left[ \begin{matrix} 0&30&30&0\\0&30&30&0\\0&30&30&0\\0&30&30&0\end{matrix}\right]\tag{2}</script><p>如果把他们转化成图片，可以很直观的看到，原图左侧是黑色，右侧为白色；而图像经过卷积以后变成中间一条黑色，两边是白色。这说明了原本黑白交接的边界线被卷积识别出来了。因此，(1)(2)中的卷积核实际上是起到了检测图片垂直边缘的作用。同样地，如果给这个卷积核加上一个90度的旋转变换，可以得到一个水平边缘检测器。不难想象，如果在神经网络中使用卷积运算，网络就可以通过拟合卷积核中的参数来检测不同方向上的边缘。这就是卷积提取图像特征的方式。</p>
<p>事实上，卷积神经网络在一个卷积层中会使用多个卷积核。每一个卷积核进行卷积运算都会得到一个二阶矩阵，这个矩阵我们把他叫做<strong>feature map</strong>(特征图)。使用n个卷积核就会形成n个feature map。最后我们会把这些feature map“叠”在一起，进入下一层计算，下一层的输入通道数即为n。</p>
<p><img alt data-src="/posts/17556c44/edge_detect.jpg" class="lazyload"></p>
<h2 id="卷积运算的常用参数"><a href="#卷积运算的常用参数" class="headerlink" title="卷积运算的常用参数"></a>卷积运算的常用参数</h2><h3 id="卷积核大小"><a href="#卷积核大小" class="headerlink" title="卷积核大小"></a>卷积核大小</h3><p>卷积核大小，顾名思义，就是卷积核矩阵的维度。通常为奇数，常见的取值有3x3，5x5和7x7。现在一般认为小卷积核的效果比较好。一个7x7的卷积，其实可以用两个3x3卷积来代替。</p>
<h3 id="Padding和Stride"><a href="#Padding和Stride" class="headerlink" title="Padding和Stride"></a>Padding和Stride</h3><p>padding（填充的层数）: 我们从(1)的例子中可以看到，做完卷积运算以后，图片变小了（从6x6变成了4x4）。这不利于进一步提取图像特征。因此，我们可以使用padding来填充图像外围区域，比如填0，来把原图变大，这样可以保证卷积后图片的大小。</p>
<p>stride（步长）：指的是在“滑动窗口”的过程中，每次滑动的步长。你可以对横向和纵向分别设置。</p>
<h2 id="卷积的好伙伴——池化（Pooling）"><a href="#卷积的好伙伴——池化（Pooling）" class="headerlink" title="卷积的好伙伴——池化（Pooling）"></a>卷积的好伙伴——池化（Pooling）</h2><p>观察(2)中的卷积结果，可以发现用卷积做边缘检测时，边界线被“弄粗”了。这个时候我们可以用池化来解决这个问题。池化也是一种非常简单的数学运算，我们还是先来看一个例子：</p>
<script type="math/tex; mode=display">\left[ \begin{matrix} 0&0&30&30&0&0\\0&0&30&30&0&0\\0&0&30&30&0&0\\0&0&30&30&0&0\\0&0&30&30&0&0\\0&0&30&30&0&0 \end{matrix} \right]\overset{2\times2MP,s=2}\longrightarrow \left[ \begin{matrix} 0&30&0\\0&30&0\\0&30&0 \end{matrix} \right] \tag{3}</script><p>还是那卷积运算中的滑动窗口的眼光来看，这里用了2x2池化，也就是用2x2的窗口去套原矩阵，并且选择2x2窗口中的最大值，组成新的矩阵。你也可以选择2x2窗口中的平均值，这是两种不同的池化方式，一个叫Max pooling（最大池化），另一个叫Average pooling（平均池化）。我们一般用缩写MP和AP来表示。  </p>
<p>池化还有一个好处，他可以把feature map缩小，减少运算数量。同时，池化是不需要参数的，也就是不参与训练，不会耗费训练时间。</p>
<h2 id="卷积神经网络（CNN）"><a href="#卷积神经网络（CNN）" class="headerlink" title="卷积神经网络（CNN）"></a>卷积神经网络（CNN）</h2><p>卷积神经网络由输入层，卷积层和全连接层构成。</p>
<h3 id="卷积层（Conv）"><a href="#卷积层（Conv）" class="headerlink" title="卷积层（Conv）"></a>卷积层（Conv）</h3><p>卷积层就是把图像对多个卷积核进行卷积，然后再对得到的feature map做池化处理。<strong>一个卷积层通常由卷积+池化组成</strong>（大概是池化没有参数，所有不配单独成池化层233333）。  </p>
<p>在卷积层中，出了刚刚提到的卷积参数以外，还有一个超参数需要配置，就是卷积核的个数。这个在上文也已有介绍。</p>
<h3 id="全连接层（FC）"><a href="#全连接层（FC）" class="headerlink" title="全连接层（FC）"></a>全连接层（FC）</h3><p>一个CNN网络通常会包含多个卷积层，然后在后面接一个或者多个全连接层。全连接层实际上就是以前接触过的普通的神经网络中的层。负责把conv层提取到的特征进行归纳计算。</p>
<h3 id="卷积层的优势"><a href="#卷积层的优势" class="headerlink" title="卷积层的优势"></a>卷积层的优势</h3><ol>
<li>卷积运算量不大，可以高效提取特征信息。</li>
<li>池化层可以大大减少运算量，加快训练速度。</li>
<li>多个卷积层可以把图片中的参数变少，比如128x128x3的图片，经过数轮卷积层以后变成7x7x128的feature map，图片参数从49152减小到6272。</li>
</ol>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined" target="_blank" rel="noopener">QF</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://blog.logan-qiu.cn/posts/17556c44/">http://blog.logan-qiu.cn/posts/17556c44/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://blog.logan-qiu.cn">QF的个人博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习    </a><a class="post-meta__tags" href="/tags/%E7%90%86%E8%AE%BA/">理论    </a><a class="post-meta__tags" href="/tags/CNN/">CNN    </a><a class="post-meta__tags" href="/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">卷积神经网络    </a></div><div class="post_share"><div class="social-share" data-image="/posts/17556c44/cover.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button"><i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="/static/wechat.jpg"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="/static/alipay.jpg"><div class="post-qr-code__desc">支付宝</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/posts/da209311/"><img class="prev_cover lazyload" data-src="/posts/da209311/liuyong.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>关于这首柳永的《鹤冲天》</span></div></a></div><div class="next-post pull_right"><a href="/posts/8ab1f019/"><img class="next_cover lazyload" data-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>Hello hexo+Butterfly</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/posts/ef2be802/" title="DenseNet论文阅读笔记"><img class="relatedPosts_cover lazyload"data-src="/posts/ef2be802/cover.png"><div class="relatedPosts_title">DenseNet论文阅读笔记</div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: 'cf8d80f66edd812018c3',
  clientSecret: '49457ffbfd77d5a3b9e192f9657bc99a46f8b6ba',
  repo: 'SaltyFishQF.github.io',
  owner: 'SaltyFishQF',
  admin: 'SaltyFishQF',
  id: md5(decodeURI(location.pathname)),
  language: ''
})
gitalk.render('gitalk-container')</script></div></div></div><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2020 By QF</div><div class="framework-info"><span>驱动 </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">简</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="/js/tw_cn.js"></script><script>translateInitilization()
</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script></body></html>